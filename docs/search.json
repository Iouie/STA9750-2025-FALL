[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stanley Louie",
    "section": "",
    "text": "Hello, my name is Stanley and I’m currently a masters student at Baruch studying Statistics. I’m expected to graduate on December 2027.\n\nEducation\nCUNY Baruch New York | M.S in Statistics | Aug 2025 - Present\nRochester Institute of Technology | B.S in New Media Interactive Development | Aug 2016 - May 2022\n\n\nProjects\n\n\n\nMini Project 01\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. In venenatis.\n\n\n\n\nMini Project 02\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus at.\n\n\n\n\nMini Project 03\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus at.\n\n\n\n\nMini Project 04\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus at.\n\n\n\n\nFinal Project\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus at.\n\n\n\n\nLast Updated: Tuesday 09 30, 2025 at 18:20PM"
  },
  {
    "objectID": "docs/mp01.html",
    "href": "docs/mp01.html",
    "title": "NETFLIX",
    "section": "",
    "text": "Last Updated: Sunday 09 28, 2025 at 17:48PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Netflix’s Most Popular Programs: An Indepth Analysis",
    "section": "",
    "text": "1) SUMMARY\nNetflix has been investing heavily in original TV series and film content with the main purpose of attracting a worldwide audience. Netflix is trying to put out a series of press releases highlighting their recent successes. The data analyzes the most popular Netflix films and TV shows between 2021 and 2025 in countries where Netflix is available and globally throughout the world.\n\n\n2) DATA EXPLORATION\nWe are gathering the data from Netflix’s TumDum Top 10, and cleaning it.\n\n\nCode\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\ninstall.packages(c(\"ggplot2\", \"dplyr\", \"readr\", \"rnaturalearth\", \"rnaturalearthdata\"))\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(ggplot2)\nlibrary(scales)\n\n\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\n# Data cleaning to replace \"N/A\" string with N/A\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME, na=c(\"N/A\"))\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na=c(\"N/A\"))\n\n\n\n\n3) FUN FACTS\n\nNetflix operates in 94 different countries.\n\n\n\nCode\nnum_countries &lt;- COUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% count()\n\n\n\nOf all the non-english Netflix films, All Quiet on the Western Front ranks number one with 23 consecutive weeks in the global top 10.\n\n\n\nCode\nnon_engtop10 &lt;- GLOBAL_TOP_10 |&gt; select(category, show_title, cumulative_weeks_in_top_10) |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  arrange(desc(cumulative_weeks_in_top_10))\n\n\n\nThe longest film to have ever appeared in Netflix global Top 10 is Pushpa P: The Rule (Reloaded Version) which has a runtime of 224 minutes.\n\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; select(show_title, category, runtime) |&gt; \n  filter(category == \"Films (Non-English)\" | category == \"Films (English)\")|&gt; \n  mutate(runtime_mins = round(runtime * 60)) |&gt; arrange(desc(runtime_mins)) |&gt; \n  distinct(show_title, category, runtime, runtime_mins) \n\nlongest_film |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(longest_film),\n              color ='white')\n\n\n\n\n\n\n\nOf the four categories of Netflix’s Global Top 10, TV (English) has the most total hours of global viewership with a staggering 66,060,030,000 weekly hour views.\n\n\n\nCode\ntotal_weekly_viewers &lt;- GLOBAL_TOP_10 %&gt;% group_by(category) %&gt;% \n  summarise(total = sum(weekly_hours_viewed)) %&gt;% arrange(desc(total))\n\n\n\nMoney Heist has the longest run in a country’s Top 10 with 127 consecutive weeks in Pakistan.\n\n\n\nCode\ntv_tops &lt;- COUNTRY_TOP_10 |&gt; \n  select(category, show_title, cumulative_weeks_in_top_10,country_name) |&gt; \n  filter(category == \"TV\") |&gt; distinct(show_title, country_name, .keep_all = TRUE) |&gt;\n  arrange(desc(cumulative_weeks_in_top_10))\n\ntv_tops |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(tv_tops),\n              color ='white')\n\n\n\n\n\n\n\nThroughout 2021 to 2025, only Russia has less than 200 weeks of service with only 35 weeks of data.\n\n\n\nCode\ncountry_removed &lt;- COUNTRY_TOP_10 |&gt; group_by(country_name) |&gt; \n  summarise(num_weeks = n_distinct(week)) |&gt; \n  arrange(num_weeks)\n\ncountry_removed |&gt; \n    head(n=3) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(country_removed),\n              color ='white')\n\n\n\n\n\n\n\nThe total viewership for the TV show Squid Game from Season 1 to Season 3 is 5,048,300,000 hours.\n\n\n\nCode\ntot_hrs_squgame &lt;- GLOBAL_TOP_10 |&gt; \n  select(season_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(season_title, \"Squid Game: Season\")) |&gt; \n  summarise(total_hrs = sum(weekly_hours_viewed))\n\n\n\nThe movie Red Notice has 201,390,863 weekly hour views in 2021.\n\n\n\nCode\nred_weekly_views &lt;- GLOBAL_TOP_10 |&gt; \n  select(week, show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(week, \"2021\"),show_title == \"Red Notice\") |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  \n#convert Red Notice film time from 1hr 58 mins to 1.97hrs\n\n  summarise(red_total_views = total_wkly_hrs/ 1.97)\n\n\n\nThere are 45 films that reached No.1 but did not originally debut with the most recent one being Unknown Number: The High School Catfish in the US.\n\n\n\nCode\n# Create a variable that stores only us films\nus_films &lt;- COUNTRY_TOP_10 |&gt; \n  select(country_iso2,week, show_title, weekly_rank, category) |&gt; \n  filter(country_iso2 == \"US\", category == \"Films\")\n \n# Get all the films that eventually hit No.1 and most recent one.\n\nus_tops &lt;- us_films %&gt;% group_by(show_title) %&gt;% \n  summarise(debut_rank = weekly_rank[which.min(week)], \n            first_hit_week = min(week[weekly_rank == 1]), \n            hit_no1 = any(weekly_rank == 1)) %&gt;% \n  filter(hit_no1, debut_rank != 1) %&gt;% \n  select(show_title, debut_rank, first_hit_week) |&gt; \n  arrange(desc(first_hit_week))\n\nus_tops |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(us_tops),\n              color ='white')\n\n\n\n\n\n\n\nThere are 7 TV shows tied that hit top 10 in the most countries during its debut week with 94 countries.\n\n\n\nCode\ndebut &lt;- COUNTRY_TOP_10 %&gt;% filter(category ==\"TV\") %&gt;% \n  group_by(show_title, country_name) %&gt;% \n  summarise(debut_week = min(week), \n            debut_rank = weekly_rank[which.min(week)], \n            .groups=\"drop\" ) %&gt;% \n  group_by(show_title) %&gt;% \n  summarise(countries_in_top_10 = n_distinct(country_name), \n            .groups= \"drop\") %&gt;% \n  arrange(desc(countries_in_top_10))\n\n\ndebut |&gt; \n    head(n=10) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(debut),\n              color ='white')\n\n\n\n\n\n\n\n\n4) ENDGAME IN THE UPSIDE DOWN: STRANGER THINGS 5 BREAKS NEW GROUND\n\n\n\nStranger Things 5\n\n\nNetflix’s cultural phenomenon, Stranger Things, will unveil their fifth and final season on November 26. Since its debut in 2016, Stranger Things has captivated and garnered a worldwide audience, jump starting the trajectory of many Netflix original series that we know and love today. From the Upside Down to the Overworld, there will be a world where Stranger Things will reclaim its throne as Netflix’s most popular TV show.\nThroughout it’s 4 seasons, Stranger Things has amassed 423,997,143 daily viewers.\n\n\nCode\nGLOBAL_TOP_10 |&gt; select(show_title, weekly_hours_viewed) |&gt; filter(show_title ==\n                                                                     'Stranger Things') |&gt;\n  group_by(show_title) |&gt; summarise(total_weekly_hrs_stranger_things = sum(weekly_hours_viewed)) |&gt;\n  mutate(daily_hours_viewed =\n           total_weekly_hrs_stranger_things / 7)\n\n\n# A tibble: 1 × 3\n  show_title      total_weekly_hrs_stranger_things daily_hours_viewed\n  &lt;chr&gt;                                      &lt;dbl&gt;              &lt;dbl&gt;\n1 Stranger Things                       2967980000         423997143.\n\n\nOut of the 94 countries that Netflix operates in, Stranger Things have been in the top 10 for 93 of them with an average of 13 consecutive weeks in the top 10.\n\n\nCode\nnum_countries_top10 &lt;- COUNTRY_TOP_10 |&gt; \n  select(country_name, show_title, cumulative_weeks_in_top_10) |&gt; \n  filter(show_title =='Stranger Things') |&gt; \n  group_by(country_name) |&gt; \n  summarise(num_countries = n_distinct(cumulative_weeks_in_top_10))\n\nweekly_consecutive_top10_avg &lt;- sum(num_countries_top10$num_countries) / 93\n\nnum_countries_top10 |&gt; \n    datatable(options=list(paging=TRUE, searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(num_countries_top10),\n              color ='white')\n\n\n\n\n\n\nComparing Stranger Things to the 7 TV shows that hit top 10 during its debut week in 94 different countries, Stranger Things ranks as the 2nd highest total weekly hours only behind the cultural phenomenon Squid Game. Nearly a decade after its debut, Stranger Things continue to prove its staying power and relevance in the ever changing landscape of TV shows. As the series heads toward its conclusion, its continued dominance among the most-watched shows reaffirm its legacy as one of Netflix’s defining franchises.\n\n\nCode\nget_7_of_top10_debuts &lt;- GLOBAL_TOP_10 |&gt; \n  select(show_title, weekly_hours_viewed) |&gt; \n  filter(show_title == \"Stranger Things\" | \n           show_title == \"All of Us Are Dead\" | \n           show_title == \"Emily in Paris\"| \n           show_title ==\"Inventing Anna\" | \n           show_title == \"Sex Education\" | \n           show_title == \"Squid Game\" | \n           show_title ==\"The Witcher\" | \n           show_title == \"You\") |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_weekly_hours = sum(weekly_hours_viewed))\n\nggplot(get_7_of_top10_debuts, aes(x = reorder(show_title, -total_weekly_hours), \n                                  y = total_weekly_hours)) + \n  geom_col(fill = \"#375b7f\") + \n  labs(title = \"Total Weekly Hour Views\",x = \"Show Title\", \n       y = \"Total Hours Viewed\") + theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))\n\n\n\n\n\n\n\n\n\n\n\n5) FROM BOLLYWOOD TO BLOCKBUSTERS: INDIA IS ON THE RISE\n\n\n\nNetflix India\n\n\nIndia is starting to become one of the big players in the market. Hindi films have started to make strides and hitting top weekly charts. The Hindi film RRR (Hindi) has been in the top 10 cumulatively for 18 weeks. The show has a weekly hour view of 79,780,000. Don’t sleep on the other films either! All Hindi films that have appeared in Netflix’s top 10 total up to 183,000,000 weekly hours viewed.\n\n\n\nCode\ntop_hindi_films &lt;- GLOBAL_TOP_10 |&gt; select(show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(show_title, \"Hindi\")) |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  slice_max(total_wkly_hrs, n = 5)\n\nggplot(top_hindi_films, aes(x= reorder(show_title, total_wkly_hrs), y=total_wkly_hrs)) +\n  geom_col(fill = \"#375b7f\") +\n  coord_flip() +\n  labs(x =\"Hindi Shows\", y = \"Weekly Hours Viewed\", title = \"Top Hindi Shows\")+\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nhighest_cum_wks &lt;- GLOBAL_TOP_10 %&gt;% \n  filter(str_detect(show_title, \"Hindi\")) |&gt; \n  group_by(show_title) |&gt; \n  slice_max(cumulative_weeks_in_top_10, n=1) %&gt;% ungroup() %&gt;%\n  arrange(desc(cumulative_weeks_in_top_10)) %&gt;%\n  slice_head(n = 5)\n\nggplot(highest_cum_wks, aes(x= reorder(show_title, -cumulative_weeks_in_top_10), \n                            y=cumulative_weeks_in_top_10)) +\n  geom_col(fill = \"#375b7f\") +\n  labs(x =\"Hindi Shows\", y = \"Cumulative Weeks in Top 10\", title = \"Top Hindi Shows\")\n\n\n\n\n\n\n\n\n\n\n\n6) WWE AND NETFLIX TAG TEAM TO BRING WRESTLING FANS UNMATCHED ACCESS\n\n\n\nNetflix WWE\n\n\nFor over 70 years, WWE has been the heart and soul of sports entertainment, spanning across generations, developing iconic superstars, and generating legendary and unforgettable moments watched by millions across the globe. On January 6, 2025, WWE will officially be streamed live on Netflix. WWE fans will now be able to stream shows at anywhere, anytime, and on any device. Thousands of new audiences have already flocked in to watch WWE’s special Pay-Per-Views resulting in WWE getting into the weekly top 10 in 42 different countries!\n\n\nCode\nwwe_tv &lt;- COUNTRY_TOP_10 %&gt;% filter(str_detect(show_title, \"WWE\"))\n\n#get count of unique countries\nwwe_tv %&gt;% summarise(unique_count = n_distinct(country_name))\n\n\n# A tibble: 1 × 1\n  unique_count\n         &lt;int&gt;\n1           42\n\n\nCode\nworld_map &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nworld_map &lt;- world_map %&gt;%\n  mutate(highlight = admin %in% wwe_tv$country_name)\n\nggplot(world_map) +\n  geom_sf(aes(fill = highlight), color = \"grey80\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"black\")) +\n  theme_minimal() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n In the map above, we can see that WWE has a strong presence in North America, South America, Europe, and parts of Asia. However, there are still many countries in Africa and the Middle East that have yet to embrace the thrill of WWE. With Netflix’s global reach, WWE has the potential to expand its audience and bring the excitement of wrestling to new regions. Speaking of regions, the graph below shows the number of times a WWE show has hit the weekly top 10. It’s not surprising that the 2 biggest events, WrestleMania and SummerSlam, are the most popular show.\n\n\nCode\ncountry_count &lt;- wwe_tv %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(num_countries = n_distinct(country_name))\n\nggplot(country_count, aes(x= reorder(show_title, num_countries), y=num_countries)) +\n  geom_col(fill = \"#375b7f\") +\n  coord_flip() +\n  labs(x =\"WWE Shows\", y = \"# of Countries that WWE Hit Top 10\", title = \"WWE Popularity\")\n\n\n\n\n\n\n\n\n\n\n\nLast Updated: Tuesday 10 21, 2025 at 21:37PM"
  },
  {
    "objectID": "mp01.html#acquiring-data",
    "href": "mp01.html#acquiring-data",
    "title": "Netflix’s Most Popular Programs: An Indepth Analysis",
    "section": "",
    "text": "if(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n----------\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(readr)\nlibrary(dplyr)\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\n---------------------\n\n\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt; mutate(season_title = if_else(season_title ==\"N/A\", NA, season_title))\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na=c(\"N/A\"))\n\n1) How many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n### create new variable to store the unique countries column and their initials. Total: 94\n\nall_countries &lt;- distinct(COUNTRY_TOP_10, country_name, country_iso2)  \n\n2) Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n### create new variable to show noneng top 10 shows, only filter category to nonenglish films, select category, show title, and cumulative weeks\n\nnon_engtop10 &lt;- GLOBAL_TOP_10 |&gt; filter(category == \"Films (Non-English)\") |&gt; select(category, show_title, cumulative_weeks_in_top_10) |&gt; arrange(desc(cumulative_weeks_in_top_10)). All Quiet on the Western Front | 23 weeks\n\n3) What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\nNote that Netflix does not provide runtime for programs before a certain date, so your answer here may be a bit limited.\n\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; select(show_title, category, runtime) |&gt; filter(category == \"Films (Non-English)\" | category == \"Films (English)\")|&gt; mutate(runtime_mins = round(runtime * 60)) |&gt; arrange(desc(runtime_mins)) |&gt; distinct(show_title, category, runtime, runtime_mins) \n\nPushpa 2: The Rule (Reloaded Version) Runtime: 224mins\n\n4) For each of the four categories, what program has the most total hours of global viewership?\n\ntotal_weekly_viewers&lt;- GLOBAL_TOP_10 %&gt;% group_by(category) %&gt;% summarise(total = sum(weekly_hours_viewed)) %&gt;% arrange(desc(total))\n\n5) Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\ntv_tops &lt;- COUNTRY_TOP_10 |&gt; select(category, show_title, cumulative_weeks_in_top_10,country_name) |&gt; filter(category == \"TV\") |&gt; distinct(show_title, country_name, .keep_all = TRUE) |&gt; arrange(desc(cumulative_weeks_in_top_10))\n\nMoney Heist: Pakistan 127 consecutive weeks\n\n7) What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\ntot_hrs_squgame &lt;- GLOBAL_TOP_10 |&gt; select(season_title, weekly_hours_viewed) |&gt; filter(str_detect(season_title, \"Squid Game: Season\")) |&gt; summarise(total_hrs = sum(weekly_hours_viewed))\n\n8) The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\nred_weekly_views &lt;- GLOBAL_TOP_10 |&gt; select(week, show_title, weekly_hours_viewed) |&gt; filter(str_detect(week, \"2021\"),show_title == \"Red Notice\") |&gt; summarise(total_wkly_hrs = sum(weekly_hours_viewed)) \n\n396740000\n\ntotal weekly hours viewed / 1.97hrs\n\nred_weekly_views &lt;- red_weekly_views / 1.97 = 201390863 total views in 2021\n\n9) How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\nus_films &lt;- COUNTRY_TOP_10 |&gt; select(country_iso2,week, show_title, weekly_rank, category) |&gt; filter(country_iso2 == \"US\", category == \"Films\") SHOW US FILMS ONLY\n\nus_tops &lt;- us_films %&gt;% group_by(show_title) %&gt;% summarise(debut_rank = weekly_rank[which.min(week)], first_hit_week = min(week[weekly_rank == 1]), hit_no1 = any(weekly_rank == 1)) %&gt;% filter(hit_no1, debut_rank != 1) %&gt;% select(show_title, debut_rank, first_hit_week) |&gt; arrange(desc(first_hit_week))\n\n10) Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\ndebut &lt;- COUNTRY_TOP_10 %&gt;% filter(category ==\"TV\") %&gt;% group_by(show_title, country_name) %&gt;% summarise(debut_week = min(week), debut_rank = weekly_rank[which.min(week)], .groups=\"drop\" ) %&gt;% group_by(show_title) %&gt;% summarise(countries_in_top_10 = n_distinct(country_name), .groups= \"drop\") %&gt;% arrange(desc(countries_in_top_10))\n\n\nLast Updated: Monday 09 29, 2025 at 21:58PM"
  },
  {
    "objectID": "mp01.html#fun-facts",
    "href": "mp01.html#fun-facts",
    "title": "Netflix’s Most Popular Programs: An Indepth Analysis",
    "section": "1.1) FUN FACTS",
    "text": "1.1) FUN FACTS\nWe are gathering the data from Netflix’s TumDum Top 10 which consists of\n\nGlobal Top 10\nCountry-wide Top 10\n\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\ninstall.packages(c(\"ggplot2\", \"dplyr\", \"readr\", \"rnaturalearth\", \"rnaturalearthdata\"))\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(ggplot2)\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\n# Data cleaning to replace \"N/A\" string with N/A\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME, na=c(\"N/A\"))\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na=c(\"N/A\"))\n\n\n\nNetflix operates in 94 different countries.\n\n\n\nCode\nnum_countries &lt;- COUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% count()\n\n\n\nOf all the non-english Netflix films, All Quiet on the Western Front spent 23 consecutive weeks in the global top 10.\n\n\n\nCode\nnon_engtop10 &lt;- GLOBAL_TOP_10 |&gt; select(category, show_title, cumulative_weeks_in_top_10) |&gt; \n  filter(category == \"Films (Non-English)\") |&gt; \n  arrange(desc(cumulative_weeks_in_top_10))\n\n\n\nThe longest film to have ever appeared in Netflix global Top 10 is Pushpa P: The Rule (Reloaded Version) which lasts for 224 minutes.\n\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; select(show_title, category, runtime) |&gt; \n  filter(category == \"Films (Non-English)\" | category == \"Films (English)\")|&gt; \n  mutate(runtime_mins = round(runtime * 60)) |&gt; arrange(desc(runtime_mins)) |&gt; \n  distinct(show_title, category, runtime, runtime_mins) \n\nlongest_film |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(longest_film),\n              color ='white')\n\n\n\n\n\n\n\nOf the four categories of Netflix’s Global Top 10, TV (English) has the most total hours of global viewership with a staggering 66,060,030,000 weekly hours viewed since 2021.\n\n\n\nCode\ntotal_weekly_viewers &lt;- GLOBAL_TOP_10 %&gt;% group_by(category) %&gt;% \n  summarise(total = sum(weekly_hours_viewed)) %&gt;% arrange(desc(total))\n\n\n\nMoney Heist had the longest run in a country’s Top 10 with 127 consecutive weeks in the top 10 in Pakistan.\n\n\n\nCode\ntv_tops &lt;- COUNTRY_TOP_10 |&gt; \n  select(category, show_title, cumulative_weeks_in_top_10,country_name) |&gt; \n  filter(category == \"TV\") |&gt; distinct(show_title, country_name, .keep_all = TRUE) |&gt;\n  arrange(desc(cumulative_weeks_in_top_10))\n\ntv_tops |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(tv_tops),\n              color ='white')\n\n\n\n\n\n\n\nThroughout 2021 to 2025, only Russia has less than 200 weeks of service with only 35 weeks.\n\n\n\nCode\ncountry_removed &lt;- COUNTRY_TOP_10 |&gt; group_by(country_name) |&gt; \n  summarise(num_weeks = n_distinct(week)) |&gt; \n  arrange(num_weeks)\n\ncountry_removed |&gt; \n    head(n=3) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(country_removed),\n              color ='white')\n\n\n\n\n\n\n\nThe total viewership for the TV show Squid Game from Season 1 to Season 3 is 5,048,300,000 hours.\n\n\n\nCode\ntot_hrs_squgame &lt;- GLOBAL_TOP_10 |&gt; \n  select(season_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(season_title, \"Squid Game: Season\")) |&gt; \n  summarise(total_hrs = sum(weekly_hours_viewed))\n\n\n\nThe movie Red Notice has 201,390,863 weekly hour views in 2021.\n\n\n\nCode\nred_weekly_views &lt;- GLOBAL_TOP_10 |&gt; \n  select(week, show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(week, \"2021\"),show_title == \"Red Notice\") |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  \n#convert Red Notice film time from 1hr 58 mins to 1.97hrs\n\n  summarise(red_total_views = total_wkly_hrs/ 1.97)\n\n\n\nThere are 45 films that reached No.1 but did not originally debut with the most recent one being Unknown Number: The High School Catfish in the US.\n\n\n\nCode\n# Create a variable that stores only us films\nus_films &lt;- COUNTRY_TOP_10 |&gt; \n  select(country_iso2,week, show_title, weekly_rank, category) |&gt; \n  filter(country_iso2 == \"US\", category == \"Films\")\n \n# Get all the films that eventually hit No.1 and most recent one.\n\nus_tops &lt;- us_films %&gt;% group_by(show_title) %&gt;% \n  summarise(debut_rank = weekly_rank[which.min(week)], \n            first_hit_week = min(week[weekly_rank == 1]), \n            hit_no1 = any(weekly_rank == 1)) %&gt;% \n  filter(hit_no1, debut_rank != 1) %&gt;% \n  select(show_title, debut_rank, first_hit_week) |&gt; \n  arrange(desc(first_hit_week))\n\nus_tops |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(us_tops),\n              color ='white')\n\n\n\n\n\n\n\nThere are 7 TV shows tied that hit top 10 in the most countries during its debut week with 94 countries.\n\n\n\nCode\ndebut &lt;- COUNTRY_TOP_10 %&gt;% filter(category ==\"TV\") %&gt;% \n  group_by(show_title, country_name) %&gt;% \n  summarise(debut_week = min(week), \n            debut_rank = weekly_rank[which.min(week)], \n            .groups=\"drop\" ) %&gt;% \n  group_by(show_title) %&gt;% \n  summarise(countries_in_top_10 = n_distinct(country_name), \n            .groups= \"drop\") %&gt;% \n  arrange(desc(countries_in_top_10))\n\n\ndebut |&gt; \n    head(n=10) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(debut),\n              color ='white')"
  },
  {
    "objectID": "mp01.html#gathering-data",
    "href": "mp01.html#gathering-data",
    "title": "Netflix’s Most Popular Programs: An Indepth Analysis",
    "section": "",
    "text": "We are gathering the data from Netflix’s TumDum Top 10 which consists of\n\nGlobal Top 10\nCountry-wide Top 10\n\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\nlibrary(readr)\nlibrary(dplyr)\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na=c(\"N/A\"))\n\n\n\nHow many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n\n\nall_countries &lt;- distinct(COUNTRY_TOP_10, country_name, country_iso2)\n\nWhich non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\n\n\nnon_engtop10 &lt;- GLOBAL_TOP_10 |&gt; filter(category == “Films (Non-English)”) |&gt; select(category, show_title, cumulative_weeks_in_top_10) |&gt; arrange(desc(cumulative_weeks_in_top_10)). All Quiet on the Western Front | 23 weeks\n\nWhat is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\nNote that Netflix does not provide runtime for programs before a certain date, so your answer here may be a bit limited.\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; select(show_title, category, runtime) |&gt; filter(category == “Films (Non-English)” | category == “Films (English)”)|&gt; mutate(runtime_mins = round(runtime * 60)) |&gt; arrange(desc(runtime_mins)) |&gt; distinct(show_title, category, runtime, runtime_mins)\nPushpa 2: The Rule (Reloaded Version) Runtime: 224mins\n\nFor each of the four categories, what program has the most total hours of global viewership?\n\ntotal_weekly_viewers&lt;- GLOBAL_TOP_10 %&gt;% group_by(category) %&gt;% summarise(total = sum(weekly_hours_viewed)) %&gt;% arrange(desc(total))\n\nWhich TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\ntv_tops &lt;- COUNTRY_TOP_10 |&gt; select(category, show_title, cumulative_weeks_in_top_10,country_name) |&gt; filter(category == “TV”) |&gt; distinct(show_title, country_name, .keep_all = TRUE) |&gt; arrange(desc(cumulative_weeks_in_top_10))\nMoney Heist: Pakistan 127 consecutive weeks\n\nWhat is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\ntot_hrs_squgame &lt;- GLOBAL_TOP_10 |&gt; select(season_title, weekly_hours_viewed) |&gt; filter(str_detect(season_title, “Squid Game: Season”)) |&gt; summarise(total_hrs = sum(weekly_hours_viewed))\n\nThe movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\nred_weekly_views &lt;- GLOBAL_TOP_10 |&gt; select(week, show_title, weekly_hours_viewed) |&gt; filter(str_detect(week, “2021”),show_title == “Red Notice”) |&gt; summarise(total_wkly_hrs = sum(weekly_hours_viewed))\n396740000\ntotal weekly hours viewed / 1.97hrs\nred_weekly_views &lt;- red_weekly_views / 1.97 = 201390863 total views in 2021\n\nHow many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\nus_films &lt;- COUNTRY_TOP_10 |&gt; select(country_iso2,week, show_title, weekly_rank, category) |&gt; filter(country_iso2 == “US”, category == “Films”) SHOW US FILMS ONLY\nus_tops &lt;- us_films %&gt;% group_by(show_title) %&gt;% summarise(debut_rank = weekly_rank[which.min(week)], first_hit_week = min(week[weekly_rank == 1]), hit_no1 = any(weekly_rank == 1)) %&gt;% filter(hit_no1, debut_rank != 1) %&gt;% select(show_title, debut_rank, first_hit_week) |&gt; arrange(desc(first_hit_week))\n\nWhich TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\ndebut &lt;- COUNTRY_TOP_10 %&gt;% filter(category ==“TV”) %&gt;% group_by(show_title, country_name) %&gt;% summarise(debut_week = min(week), debut_rank = weekly_rank[which.min(week)], .groups=“drop” ) %&gt;% group_by(show_title) %&gt;% summarise(countries_in_top_10 = n_distinct(country_name), .groups= “drop”) %&gt;% arrange(desc(countries_in_top_10))\n```\n\nLast Updated: Monday 09 29, 2025 at 22:29PM"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "",
    "text": "Housing affordability is an increasing challenge to many metropolitan areas. Rent growth has begun to outpace income, causing financial strain on many households. In this mini-project, we will explore data from the US Census Bureau and the Bureau of Labor Statistics to analyze housing affordability trends across different metropolitan areas in the United States. We will compute rent burden metrics, housing growth metrics, and identify metropolitan areas that have successfully managed to provide affordable housing options for their residents."
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "",
    "text": "Housing affordability is an increasing challenge to many metropolitan areas. Rent growth has begun to outpace income, causing financial strain on many households. In this mini-project, we will explore data from the US Census Bureau and the Bureau of Labor Statistics to analyze housing affordability trends across different metropolitan areas in the United States. We will compute rent burden metrics, housing growth metrics, and identify metropolitan areas that have successfully managed to provide affordable housing options for their residents."
  },
  {
    "objectID": "mp02.html#data-acquisition-and-preparation",
    "href": "mp02.html#data-acquisition-and-preparation",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Data Acquisition and Preparation",
    "text": "Data Acquisition and Preparation\nGrabbing data from US Census Bureau.\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(gghighlight)\nlibrary(scales)\nlibrary(DT)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\nManually grabbing number of new housing units built each year\n\n\nCode\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\n\nBLS data NAICS coding system\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    \n    if(!file.exists(fname)){\n    \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code)\n    \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n    \n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\nBLS QUARTERLY CENSUS\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\nJOINS household, household income, population, rent\n\n\nCode\ncombined_data &lt;- inner_join(HOUSEHOLDS, INCOME, by=c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n    inner_join(POPULATION, by=c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n    inner_join(RENT, by=c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n    inner_join(PERMITS, by=c(\"GEOID\" = \"CBSA\", \"year\")) |&gt;\n    mutate(std_cbsa = paste0(\"C\", GEOID))\n\nWAGES &lt;- WAGES |&gt;\n    mutate(std_cbsa = paste0(FIPS, \"0\"))"
  },
  {
    "objectID": "mp02.html#exploratory-analysis",
    "href": "mp02.html#exploratory-analysis",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nWhich CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\nlargest_units &lt;- combined_data |&gt;\n    filter(year &gt;= 2010, year &lt;= 2019) |&gt;\n    group_by(GEOID) |&gt;\n    summarize(total_permitted = sum(new_housing_units_permitted, na.rm=TRUE)) |&gt;\n    arrange(desc(total_permitted)) |&gt;\n    slice_head(n=1) \n\n\n\nFrom 2010 to 2019, the CBSA that permitted the largest number of new housing units is\n26420 with a total of 482,075 units permitted.\n\nIn what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nabq_max_year &lt;- combined_data |&gt;\n    filter(GEOID == 10740) |&gt;\n    arrange(desc(new_housing_units_permitted)) |&gt;\n    slice_head(n=1)\n\n\n\nAlbuquerque, NM (CBSA Number 10740) permitted the most new housing units in the year 2021 with 4,021 units permitted.\n\nWhich state (not CBSA) had the highest average individual income in 2015? To answer this question, you will need to first compute the total income per CBSA by multiplying the average household income by the number of households, and then sum total income and total population across all CBSAs in a state. With these numbers, you can answer this question.\n\n\nCode\nstate_income_2015 &lt;- combined_data |&gt;\n    filter(year == 2015) |&gt;\n    mutate(total_income = household_income * households) |&gt;\n    mutate(state = str_extract(NAME, \", (.{2})\", group=1)) |&gt;\n    group_by(state) |&gt;\n    summarize(total_income = sum(total_income, na.rm=TRUE),\n              total_population = sum(population, na.rm=TRUE)) |&gt;\n    mutate(average_individual_income = total_income / total_population) |&gt;\n    arrange(desc(average_individual_income)) |&gt;\n    slice_head(n=1)\n\n\n\nThe state with the highest average individual income in 2015 is DC with an average individual income of $33,232.88.\n\nData scientists and business analysts are recorded under NAICS code 5182. What is the last year in which the NYC CBSA had the most data scientists in the country? In recent, the San Francisco CBSA has had the most data scientists. For this question, you may simply create a table of which CBSA had the most data scientists each year and then answer the question in the following text.\n\n\nCode\nareas &lt;- combined_data |&gt; select(NAME,std_cbsa)\n\ndata_sci &lt;- WAGES |&gt; \n    filter(INDUSTRY == 5182) |&gt; \n    group_by(YEAR,std_cbsa) |&gt; filter(EMPLOYMENT !=0) |&gt; \n    summarise(max_employment = max(EMPLOYMENT, na.rm = TRUE)) |&gt; \n    slice_max(order_by = max_employment)\n\n#join table to get the city\ndata_sci &lt;- inner_join(data_sci, areas, by = \"std_cbsa\") |&gt; distinct()\n\n\n\nThe last year in which the NYC CBSA had the most data scientists in the country was 2015.\n\nWhat fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nCode\n# get the sum of total wages in NYC cbsa per year\ntotal_wages_sum &lt;- WAGES |&gt; \n    filter(std_cbsa == \"C35620\") |&gt; group_by(YEAR) |&gt; summarise(sum_total_wages = sum(TOTAL_WAGES))\n\n# get the sum of financial wages in NYC cbsa per year\nfinancial_wages_sum &lt;- WAGES |&gt; \n    filter(std_cbsa == \"C35620\", str_starts(as.character(INDUSTRY), \"52\")) |&gt; \n    group_by(YEAR) |&gt; summarise(sum_financial_wages = sum(TOTAL_WAGES))\n\n# join both and calculate fraction\nwage_fraction &lt;- inner_join(total_wages_sum, financial_wages_sum, by = \"YEAR\") |&gt;\n    mutate(fraction_of_financial_wages = sum_financial_wages / sum_total_wages) |&gt; arrange(desc(fraction_of_financial_wages))\n\n# getting the total fraction\ntotal_fraction &lt;- wage_fraction |&gt; summarise(fraction_total = sum(sum_financial_wages) / sum(sum_total_wages) * 100)\n\n\n\nThe fraction of total wages in the NYC CBSA earned by people employed in the finance and insurance industries is 13.29%. This fraction peaked in the year 2021 with a fraction of 15.45%."
  },
  {
    "objectID": "mp02.html#final-insights-and-deliverable",
    "href": "mp02.html#final-insights-and-deliverable",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Final Insights and Deliverable",
    "text": "Final Insights and Deliverable\nCode related to the final deliverable of the assignment goes here.\n\nThis work ©2025 by iouie was initially prepared as a Mini-Project for STA 9750 at Baruch College. More details about this course can be found at the course site and instructions for this assignment can be found at MP #02"
  },
  {
    "objectID": "mp02.html#task-3",
    "href": "mp02.html#task-3",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Task 3",
    "text": "Task 3\nThe relationship between monthly rent and average household income per CBSA in 2009.\n\n\nCode\np1 &lt;- ggplot(combined_data |&gt; filter(year == 2009), \n             aes(x = household_income, y = monthly_rent)) +\n    geom_point(alpha = 0.6, color=\"blue\") +\n    geom_smooth(method = \"lm\", color = \"#F7B800\") +\n    labs(title = \"Relationship between Monthly Rent and Average Household Income (2009)\",\n         x = \"Average Household Income\",\n         y = \"Monthly Rent\",\n        caption = \"Source: US Census Bureau ACS\") +\n    theme_grey( base_size = 18) + \n    scale_x_continuous(labels = scales::dollar_format()) +\n    scale_y_continuous(labels = scales::dollar_format()) +\n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\nprint(p1)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThe scatter plot shows a strong positive relationship between household income and monthly rent in 2009. Metro areas tend to have higher average household incomes which results in higher monthly rents. The linear regression line indicates that as household income increases, monthly rent also tends to increase.\n\nThe relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs. Design your visualization so that it is possible to see the evolution of this relationship over time.\n\n\nCode\n# get the total employment per year per cbsa\nemp_total &lt;- WAGES |&gt; \n    group_by(YEAR) |&gt; \n  summarise(total_employment = sum(EMPLOYMENT)) |&gt; \n    ungroup()\n\n# get total employment of health\nhealth_total &lt;- WAGES |&gt; \n    filter(str_starts(as.character(INDUSTRY), \"62\")) |&gt; \n    group_by(YEAR) |&gt; \n    summarise(health_employment = sum(EMPLOYMENT)) |&gt;\n    ungroup()\n# join both and calculate fraction\ngrouped_emp &lt;- inner_join(emp_total, health_total, by = \"YEAR\") |&gt; \n    mutate(fraction_health_employment = health_employment / total_employment)\n\np2 &lt;- ggplot(grouped_emp, aes(x= YEAR, y =fraction_health_employment)) +\n    geom_line(size = 0.8, color = \"#F7B800\") +\n    geom_point(color = \"blue\") +\n    labs(title = \"Total and Health Employment Percantage over time\",\n         x = \"Year\",\n         y = \"Percentage of Health Employment to Total Employment\",\n         caption = \"Source: BLS QCEW\") +\n    theme_grey(base_size = 18) + \n    scale_y_continuous(labels = scales::percent_format(accuarcy = 1)) +\n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nCode\nprint(p2)\n\n\n\n\n\n\n\n\n\n\nThe plot shows an increase of percentage of Health Employees to Total Employees over time. This indicates that the health care and social services sector has been growing in the economy. There was noticeable growth from 2019 to 2021 due to the pandemic.\n\nThe evolution of average household size over time. Use different lines to represent different CBSAs.\n\n\nCode\np3 &lt;- ggplot(combined_data, aes(x= year, y = population / households, \n                                group = GEOID, \n                                color =case_when(GEOID == 35620 ~ \"New York\",\n                                                 GEOID == 31080 ~ \"Los Angeles\"))) +\n    geom_line(linewidth = 0.8) +\n   scale_color_manual(values = c(\"New York\" = \"#F7B800\", \"Los Angeles\" = \"green\"),\n                     name = \"City\") +\n  gghighlight(GEOID %in% c(35620, 31080), \n              use_direct_label = FALSE, \n              unhighlighted_params = list(alpha=0.5)) +\n    labs(title = \"Average Household Size over Time by CBSA\",\n         x = \"Year\",\n         y = \"Average Household Size\",\n         caption = \"Source: US Census Bureau ACS\") +\n    theme_grey(base_size = 18) + \n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nCode\nprint(p3)\n\n\n\n\n\n\n\n\n\n\nThe plot shows the average household size over time for different CBSAs, with a focus on New York and Los Angeles. Both cities show a slight decrease in average household size over the years, indicating a trend towards smaller households in these metropolitan areas."
  },
  {
    "objectID": "mp02.html#rent-burden",
    "href": "mp02.html#rent-burden",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Rent Burden",
    "text": "Rent Burden\n\n\nCode\nrent_burden_data &lt;- inner_join(INCOME, RENT, by=c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n    mutate(yearly_rent = monthly_rent * 12,\n    rent_burden = yearly_rent / household_income,\n    rent_burden_percent = yearly_rent/ household_income * 100)\n\n# scaling it\nlowest_burden &lt;- min(rent_burden_data$rent_burden_percent, na.rm = TRUE)\nhighest_burden &lt;- max(rent_burden_data$rent_burden_percent, na.rm = TRUE)\n\n# creating rbi\nrent_burden_data &lt;- rent_burden_data |&gt;\n    mutate(rbi = 100 * (rent_burden_percent - lowest_burden ) / (highest_burden - lowest_burden))\n\n\n\nRent burden is used to measure percentage of income spent on rent. I’ve standardized the metric so that 0 would be the lowest burden and 100 would be the highest in the observed data."
  },
  {
    "objectID": "mp02.html#metropolitan-california",
    "href": "mp02.html#metropolitan-california",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Metropolitan California",
    "text": "Metropolitan California\n\n\nCode\ncali_rent_burden &lt;- rent_burden_data |&gt;\n    filter(str_detect(NAME, \"Los Angeles\")) |&gt;\n  select( 'Year' = year,\n          'Monthly Rent' = monthly_rent,\n          'Household Income' = household_income,\n          'Rent Burden Percent' = rent_burden_percent,\n          'Rent Burden Score' = rbi) |&gt; \n  mutate(`Monthly Rent` = scales::dollar(`Monthly Rent`),\n         `Household Income` = scales::dollar(`Household Income`),\n         `Rent Burden Percent` = scales::percent(`Rent Burden Percent` / 100, accuracy = 0.1),\n         `Rent Burden Score` = round(`Rent Burden Score`, 2))\n\n\n\ncali_table &lt;- datatable(cali_rent_burden,\n          class = \"table table-striped table-hover table-dark table-sm\",\n          caption = \"Los Angeles Rent Burden 2009-2023\",\n          options = list(pageLength = 15,\n                         autoWidth = TRUE,\n                         searching = FALSE))\n\nprint(cali_table)\n\n\n\nLos Angeles has seen a small increase in rent burden from 2009 to 2023, with the Rent Burden Score rising from approximately 47.26 in 2009 to 52.91 in 2023 while drastically increasing in Household Income. The rent burden has been in the range from 24% to 26%."
  },
  {
    "objectID": "mp02.html#highest-and-lowest-metro-burden-areas",
    "href": "mp02.html#highest-and-lowest-metro-burden-areas",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Highest and Lowest Metro Burden Areas",
    "text": "Highest and Lowest Metro Burden Areas\n\n\nCode\nlow_burden &lt;- rent_burden_data |&gt;\n    filter(year == 2023, str_detect(NAME, \"Metro\")) |&gt;\n    select(\n      'City' = NAME, \n      'Year' = year, \n      'Rent Burden Percent' = rent_burden_percent, \n      'Rent Burden Score' = rbi) |&gt;\n    mutate(`Rent Burden Percent` = scales::percent(`Rent Burden Percent` / 100, accuracy = 0.1),\n           `Rent Burden Score` = round(`Rent Burden Score`, 2)) |&gt;\n    arrange(desc(`Rent Burden Score`)) |&gt; arrange(`Rent Burden Score`) |&gt;\n    head(10) \n    \ndatatable(\n  low_burden,\n  caption = \"Top 10 Lowest Rent Burden Metro Areas in 2023\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = FALSE)\n)\n\n\n\n\n\n\n\nThe lowest rent burden score is Bismarck, North Dakota with a 5.27 rent burden score with residents spending only 13.7% of their income on rent."
  },
  {
    "objectID": "mp02.html#lowest-metro-burden-areas",
    "href": "mp02.html#lowest-metro-burden-areas",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Lowest Metro Burden Areas",
    "text": "Lowest Metro Burden Areas\n\n\nCode\nlow_burden &lt;- rent_burden_data |&gt;\n    filter(year == 2023, str_detect(NAME, \"Metro\")) |&gt;\n    select(\n      'City' = NAME, \n      'Year' = year, \n      'Rent Burden Percent' = rent_burden_percent, \n      'Rent Burden Score' = rbi) |&gt;\n    mutate(`Rent Burden Percent` = scales::percent(`Rent Burden Percent` / 100, accuracy = 0.1),\n           `Rent Burden Score` = round(`Rent Burden Score`, 2)) |&gt;\n    arrange(desc(`Rent Burden Score`)) |&gt; arrange(`Rent Burden Score`) |&gt;\n    head(10) \n    \ndatatable(\n  low_burden,\n  caption = \"Top 10 Lowest Rent Burden Metro Areas in 2023\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = FALSE)\n)\n\n\n\nThe lowest rent burden score is Bismarck, North Dakota with a 5.27 rent burden score with residents spending only 13.7% of their income on rent."
  },
  {
    "objectID": "mp02.html#housing-growth",
    "href": "mp02.html#housing-growth",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Housing Growth",
    "text": "Housing Growth\n\n\nCode\ntask5_table &lt;- POPULATION |&gt;\n  inner_join(PERMITS, by=c(\"year\", 'GEOID' = 'CBSA'))\n\n# Calculate 5-year growth using lag function\npopulation_growth_5yr &lt;- task5_table |&gt;\n  arrange(NAME, year) |&gt;\nmutate(\n  pop_5yrs_ago = lag(population, 5),\n  pop_growth_5yrs = population - lag(population, 5),\n  pop_growth_5yr_percent = (population - lag(population, 5)) / lag(population, 5) * 100,\n  permits_per_1000 = (new_housing_units_permitted / population) * 1000) |&gt;\n  ungroup()\n\n# Average permits per 1000 in 2014 as baseline\nhousing_growth &lt;- population_growth_5yr |&gt;\n  filter(year == 2014) |&gt;\n  summarise(baseline = mean(permits_per_1000, na.rm = TRUE)) |&gt;\n  pull(baseline)\n\n# Index where 100 = 2014 baseline\npopulation_growth_5yr &lt;- population_growth_5yr |&gt;\n  mutate(\n    instantaneous_score = (permits_per_1000 / housing_growth) * 100\n  )\n\npop_table &lt;- population_growth_5yr |&gt;\n  select(\n    'City' = NAME,\n    'Year' = year,\n    'Population (5 yrs Ago)' = pop_5yrs_ago,\n    'New Housing Units Permitted' = new_housing_units_permitted,\n    'Population Growth (5 yr)' = pop_growth_5yrs,\n    'Population Growth (5 yr %)' = pop_growth_5yr_percent,\n    'Permits per 1000 People' = permits_per_1000,\n    'Instantaneous Score' = instantaneous_score\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(\n    `Population Growth (5 yr %)` = scales::percent(`Population Growth (5 yr %)` / 100, accuracy = 0.1),\n    `Permits per 1000 People` = round(`Permits per 1000 People`, 2),\n    `Instantaneous Score` = round(`Instantaneous Score`, 2)\n  )\n\n\ndatatable(\n  pop_table,\n  caption = \"Instantaenous Scores\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = FALSE),\n)\n\n\n\n\n\n\n\nInstantaneous Scores above 100, Faster building housing than 2014 average\nInstantaneous Scores below 100, Slower building housing than 2014 average\nInstantaneous Scores 100, Housing building matches 2014 average"
  },
  {
    "objectID": "mp02.html#rate-base-housing",
    "href": "mp02.html#rate-base-housing",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Rate Base Housing",
    "text": "Rate Base Housing\n\n\nCode\n# Declining Pop\npermits_per_new_resident &lt;- population_growth_5yr |&gt;\n  mutate(permits_per_new_resident = case_when(\n    pop_growth_5yrs &lt;= 0 ~ permits_per_1000,\n    TRUE ~ new_housing_units_permitted / pop_growth_5yrs),\n    \n    # Prevent outliers\n  cap_ratio = pmin(permits_per_new_resident, 10))\n\n# Baseline 2014 avg\nbaseline_rate &lt;-permits_per_new_resident |&gt;\n  filter(year &gt;= 2014, pop_growth_5yrs &gt; 0) |&gt;\n  summarise(baseline = mean(cap_ratio, na.rm = TRUE)) |&gt;\n  pull(baseline)\n\n# Index where 100 = 2014 baseline\npermits_per_new_resident &lt;- permits_per_new_resident |&gt;\n  mutate(rate_score = cap_ratio / baseline_rate * 100)\n\nrate_table &lt;- permits_per_new_resident |&gt;\n  filter(year &gt;= 2014) |&gt;\n  select(\n    'City' = NAME,\n    'Year' = year,\n    'Population (5 yrs Ago)' = pop_5yrs_ago,\n    'New Housing Units Permitted' = new_housing_units_permitted,\n    'Ratio of Housing Units to Population Growth' = cap_ratio,\n    'Permits per 1000 People' = permits_per_1000,\n    'Rate Score' = rate_score,\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(\n    `Permits per 1000 People` = round(`Permits per 1000 People`, 2),\n    `Ratio of Housing Units to Population Growth` = round(`Ratio of Housing Units to Population Growth`, 2),\n    `Rate Score` = round(`Rate Score`, 2)\n  )\n\n\ndatatable(\n  rate_table,\n  caption = \"Rate Scores\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = FALSE),\n)\n\n\n\nRate Scores above 100, Faster building housing than 2014 average population growth\nRate Scores below 100, Slower building housing than 2014 average population growth\nRate Scores 100, Housing building matches 2014 average population growth\nThere are many different states with extremely high Rate Scores. On the other hand, Connecticut, New York, and Ohio Metro areas have some of the lowest Rate Scores."
  },
  {
    "objectID": "mp02.html#composite-score",
    "href": "mp02.html#composite-score",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Composite Score",
    "text": "Composite Score\n\n\nCode\ncomposite_scores &lt;- permits_per_new_resident |&gt;\n  filter(year &gt;= 2014) |&gt;\n  mutate(composite_score = (instantaneous_score + rate_score) / 2) |&gt;\n  drop_na() |&gt;\n  select(\n    'City' = NAME,\n    'Year' = year,\n    'Instantaneous Score' = instantaneous_score,\n    'Rate Score' = rate_score,\n    'Composite Score' = composite_score\n  ) |&gt;\n    mutate(\n    `Instantaneous Score` = round(`Instantaneous Score`, 2),\n    `Composite Score` = round(`Composite Score`, 2),\n    `Rate Score` = round(`Rate Score`, 2)) |&gt;\n  arrange(desc(`Composite Score`))\n  \ndatatable(\n  composite_scores,\n  caption = \"Composite Score\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = FALSE),\n)\n\n\n\nSalisbury, Maryland has the highest Composite Score, while Morgantown, West Virginia has the lowest Composite Score."
  },
  {
    "objectID": "mp02.html#instantaneous-housing-growth",
    "href": "mp02.html#instantaneous-housing-growth",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Instantaneous Housing Growth",
    "text": "Instantaneous Housing Growth\n\n\nCode\ntask5_table &lt;- POPULATION |&gt;\n  inner_join(PERMITS, by=c(\"year\", 'GEOID' = 'CBSA'))\n\n# Calculate 5-year growth using lag function\npopulation_growth_5yr &lt;- task5_table |&gt;\n  arrange(NAME, year) |&gt;\nmutate(\n  pop_5yrs_ago = lag(population, 5),\n  pop_growth_5yrs = population - lag(population, 5),\n  pop_growth_5yr_percent = (population - lag(population, 5)) / lag(population, 5) * 100,\n  permits_per_1000 = (new_housing_units_permitted / population) * 1000) |&gt;\n  ungroup()\n\n# Average permits per 1000 in 2014 as baseline\nhousing_growth &lt;- population_growth_5yr |&gt;\n  filter(year == 2014) |&gt;\n  summarise(baseline = mean(permits_per_1000, na.rm = TRUE)) |&gt;\n  pull(baseline)\n\n# Index where 100 = 2014 baseline\npopulation_growth_5yr &lt;- population_growth_5yr |&gt;\n  mutate(\n    instantaneous_score = (permits_per_1000 / housing_growth) * 100\n  )\n\npop_table &lt;- population_growth_5yr |&gt;\n  filter(year &gt;= 2014) |&gt;\n  select(\n    'City' = NAME,\n    'Year' = year,\n    'Population (5 yrs Ago)' = pop_5yrs_ago,\n    'New Housing Units Permitted' = new_housing_units_permitted,\n    'Population Growth (5 yr)' = pop_growth_5yrs,\n    'Population Growth (5 yr %)' = pop_growth_5yr_percent,\n    'Permits per 1000 People' = permits_per_1000,\n    'Instantaneous Score' = instantaneous_score\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(\n    `Population Growth (5 yr %)` = scales::percent(`Population Growth (5 yr %)` / 100, accuracy = 0.1),\n    `Permits per 1000 People` = round(`Permits per 1000 People`, 2),\n    `Instantaneous Score` = round(`Instantaneous Score`, 2)\n  )\n\n\ndatatable(\n  pop_table,\n  caption = \"Instantaenous Scores\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = FALSE),\n)\n\n\n\nInstantaneous Scores above 100, Faster building housing than 2014 average\nInstantaneous Scores below 100, Slower building housing than 2014 average\nInstantaneous Scores 100, Housing building matches 2014 average\nFlorida and North/South Carolina Metro Areas have some of the highest Instantaneous Scores, occupying 9 of the top 10 spots, while Illinois, West Virginia, and Arkansas have some of the lowest Instataneous Scores."
  },
  {
    "objectID": "mp02.html#task-6-visualization",
    "href": "mp02.html#task-6-visualization",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Task 6: Visualization",
    "text": "Task 6: Visualization\n\n\nCode\nyimby1_data &lt;- rent_burden_data |&gt;\n  group_by(GEOID, NAME) |&gt;\n  filter(year &gt;= 2014) |&gt; \n  filter(!str_detect(str_trim(NAME), regex(\"PR Metro Area\", ignore_case= TRUE))) |&gt;\n summarize(\n   rent_burden_early = rent_burden_percent[year == 2014],\n   rent_burden_latest = rent_burden_percent[year == max(year)],\n   .groups = \"drop\"\n ) |&gt;\n  mutate(\n   rent_burden_change = rent_burden_latest - rent_burden_early)\n\n#threshold for high early burden\nearly_med &lt;- median(yimby1_data$rent_burden_early, na.rm = TRUE)\n\nyimby1_data &lt;- yimby1_data |&gt;\n  mutate(\n    high_early = !is.na(rent_burden_early) & (rent_burden_early &gt; early_med),\n    decreased_burden = !is.na(rent_burden_change) & (rent_burden_change &lt; 0),\n    category = case_when(\n      high_early & decreased_burden ~ \"High Early, Decreased\",\n      high_early & !decreased_burden ~ \"High Early, Not Decreased\",\n      !high_early & decreased_burden ~ \"Low Early, Decreased\",\n      TRUE ~ \"Low Early, Not Decreased\"\n    ),\n      mutate(across(c(rent_burden_early, rent_burden_latest, rent_burden_change),\n                ~ {\n                  x &lt;- as.numeric(.)\n                  if (all(is.na(x))) return(x)\n                  if (max(abs(x), na.rm = TRUE) &lt;= 1) x &lt;- x * 100\n                  round(x, 2)\n                }))\n  )\n\ncat_colors &lt;- c(\n  \"High Early, Decreased\" = \"#1a9850\",   # green\n  \"High Early, Not Decreased\"       = \"#fc8d59\",   # orange\n  \"Low Early, Decreased\"        = \"#91cf60\",   # light green\n  \"Low Early, Not Decreased\"               = \"grey80\"\n)\n\nyimby_graph1 &lt;- ggplot(yimby1_data, aes(x= rent_burden_early, y = rent_burden_change)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = early_med, linetype = \"dashed\", color = \"black\") +\n  geom_point(aes(color = category), size = 4, alpha = 0.7) +\n  scale_color_manual(values = cat_colors, name = \"Category\") +\n  scale_x_continuous(labels = label_number(suffix = \"%\", accuracy = 0.1)) +\n  scale_y_continuous(labels = label_number(suffix = \"%\", accuracy = 0.1)) +\n  labs(\n    x = paste0(\"Early Rent Burden (2014)%\"),\n    y = \"Change in Rent Burden (2023 to 2014)%\",\n    title = \"Early rent burden vs change in rent burden\") +\n  theme_grey(base_size = 18) +\n  theme(legend.position = \"bottom\",\n    legend.title = element_text(size = 10),\n    legend.text  = element_text(size = 5),\n    legend.key.size = unit(0.8, \"lines\"),\n    legend.spacing.x = unit(0.2, \"cm\"))\n\nprint(yimby_graph1)\n\n\n\n\n\n\n\n\n\nCode\ndatatable(\n  yimby1_data,\n  caption = \"Rent Over Time\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = TRUE),\n)\n\n\n\nThis scatter plot visualizes the relationship between early rent burden in 2014 and the change in rent burden up to 2023 for various CBSAs. The dashed lines indicate the median early rent burden and zero change in rent burden. CBSAs in the bottom-right show a desirable YIMBY characteristic of high early rent burden with a decrease over time.\n\n\n\nCode\n# have had population growth over the study period and have had above-average housing growth during the study period.\n# finding pop growth from 2014 - 2023\npop_for_yimby &lt;- population_growth_5yr |&gt;\n  filter(year %in% c(2014, 2023)) |&gt;\n  select(GEOID, NAME, year, population) |&gt;\n  group_by(GEOID, NAME, year) |&gt;\n  summarize(population = mean(population, na.rm = TRUE)) |&gt;\n  pivot_wider(names_from = year, values_from = population, names_prefix = \"pop_\") |&gt;\n  drop_na(pop_2014, pop_2023) |&gt;\n  mutate(\n    pop_growth = pop_2023 - pop_2014,\n    pop_growth_percentage = if_else(!is.na(pop_2014) & pop_2014 != 0,\n                             100 * (pop_2023 - pop_2014) / pop_2014,\n                             NA_real_),\n    grew_2014_2023 = if_else(!is.na(pop_growth) & pop_growth &gt; 0, TRUE, FALSE)\n  ) \n\ncombined_data &lt;- pop_for_yimby |&gt;\n  inner_join(permits_per_new_resident |&gt;\n              select(GEOID, permits_per_1000), by = \"GEOID\") |&gt;\n  distinct(GEOID, .keep_all = TRUE)\n\n\nyimby_data2 &lt;- combined_data |&gt;\n  filter(!is.na(pop_growth) & !is.na(permits_per_1000)) |&gt;\nmutate(\n  pop_grew = pop_growth &gt; 0,\n  above_avg_housing = permits_per_1000 &gt; median(combined_data$permits_per_1000, na.rm = TRUE),\n  category = case_when(\n    pop_grew & above_avg_housing ~ \"Grew Population, Above Avg Housing\",\n    pop_grew & !above_avg_housing ~ \"Grew Population, Below Avg Housing\",\n    !pop_grew & above_avg_housing ~ \"Declined Population, Above Avg Housing\",\n    TRUE ~ \"Declined Population, Below Avg Housing\"\n  ),\n  category= factor(category, levels=c(\"Grew Population, Above Avg Housing\",\n                                      \"Grew Population, Below Avg Housing\",\n                                      \"Declined Population, Above Avg Housing\",\n                                      \"Declined Population, Below Avg Housing\"))\n) |&gt;\nselect(-grew_2014_2023,-pop_grew, -above_avg_housing) |&gt;\nmutate(across(c(pop_growth_percentage, permits_per_1000),\n                ~ {\n                  x &lt;- as.numeric(.)\n                  if (all(is.na(x))) return(x)\n                  if (max(abs(x), na.rm = TRUE) &lt;= 1) x &lt;- x * 100\n                  round(x, 2)\n                }))\n\ncat_colors &lt;- c(\n  \"Grew Population, Above Avg Housing\" = \"#1a9850\",\n  \"Grew Population, Below Avg Housing\" = \"#91cf60\",\n  \"Declined Population, Above Avg Housing\" = \"#fc8d59\",\n  \"Declined Population, Below Avg Housing\" = \"grey70\"\n)\n\nyimby_graph2 &lt;- ggplot(yimby_data2, aes(x= permits_per_1000, y = pop_growth_percentage, color = category)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = median(yimby_data2$permits_per_1000, na.rm = TRUE), linetype = \"dashed\", color = \"black\") +\n  geom_point(size = 4, alpha = 0.7) +\n  scale_color_manual(values = cat_colors, name = \"Category\") +\n  scale_x_continuous(labels = label_number(suffix = \"\", accuracy = 0.1)) +\n  scale_y_continuous(labels = label_number(suffix = \"\", accuracy = 1)) +\n  labs(\n    x = paste0(\"Permits per 1000 People\"),\n    y = \"Population Growth (2014 to 2023)%\",\n    title = \"Housing Growth vs Population Growth\") +\n  theme_grey(base_size = 18) +\n  theme(legend.position = \"bottom\",\n    legend.title = element_text(size = 10),\n    legend.text  = element_text(size = 3),\n    legend.key.size = unit(0.8, \"lines\"),\n    legend.spacing.x = unit(0.2, \"cm\"))\n\nprint(yimby_graph2)\n\n\n\n\n\n\n\n\n\nCode\ndatatable(\n  yimby_data2,\n  caption = \"Housing Growth vs Population Growth\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = TRUE),\n)\n\n\n\nThis scatter plot visualizes the relationship between housing growth (measured as permits per 1000 people in 2023) and population growth from 2014 to 2023 for various CBSAs. The dashed lines indicate the median housing growth and zero population growth. CBSAs in the top-right quadrant show desirable YIMBY characteristics of both population growth and above-average housing growth."
  },
  {
    "objectID": "mp02.html#policy-brief",
    "href": "mp02.html#policy-brief",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Policy Brief",
    "text": "Policy Brief\n\nIntroduction\n\nHousing affordability is a critical issue in many metropolitan areas across the United States. Rent prices are growing faster than incomes, leading to increased rent burden for many households. This is not a sustainable situation and requires immediate attention from policymakers. Despite the challenges, some metropolitan areas have successfully implemented policies that have led to decreased rent burden while still experiencing population growth. These areas can serve as models for other cities facing similar issues.\n\n\n\nProposed Sponsors\n\nWe would like to reach out to two sponsors for this policy brief: a sponsor from Columbus, GA-AL Metropolitan Area in a YIMBY zone and a co-sponsor from Fresno, CA Metropolitan Area in a NIMBY zone. Both these metropolitan areas have successfully lowered rent burdens while growing in population and opportunities when it comes to housing affordability. By collaborating with sponsors from both YIMBY and NIMBY zones, we can gain a comprehensive understanding of the issue and develop effective strategies to address it across America.\n\n\n\nBenefits\n\nTrade, Transportation, and Utilities (NAICS 42-49)\n\nBy supporting affordable housing initiatives in Columbus, GA-AL Metropolitan Area and Fresno, CA Metropolitan Area, we can ensure that workers in the Trade, Transportation, and Utilities sector have access to affordable housing options. This will not only improve their quality of life but also enhance their productivity and job satisfaction, leading to a more robust economy in these metropolitan areas. Lower housing costs and reducing rent near transit reduces commute time and stress, which improves overall well-being and reliability. Recruitment agencies will save money on recruitment and retention costs as employees are more likely to stay in jobs where they can afford to live comfortably.\n\n\n\nHealthcare and Social Assistance (NAICS 62)\n\nAffordable housing is crucial for healthcare and social assistance workers, who often face high rent burdens. By investing in affordable housing initiatives in Columbus, GA-AL Metropolitan Area and Fresno, CA Metropolitan Area, we can ensure that these essential workers have access to safe and affordable housing options. This will not only improve their quality of life but also enhance their ability to provide quality care to the community. Affordable housing near healthcare facilities can lead to better health outcomes for both workers and patients, as it reduces stress and improves access to care. This can result in lower healthcare costs and improved public health in these metropolitan areas.\n\n\n\n\nMetrics\n\nRent Burden Index (RBI): is a 0-100 index scale using min-max normalization to standardize rent burden across CBSAs using 2014 as a baseline. A score of 0 indicates the lowest rent burden observed, which is a good thing, while a score of 100 represents the highest rent burden meaning too much income is being spent on rent. This metric allows for easy comparison of rent burdens across different metropolitan areas.\nInstantaneous Housing Growth Score: This score measures the rate of housing growth in a metropolitan area compared to the average housing growth in 2014. A score above 100 indicates that the area is building housing at a faster rate than the 2014 average, while a score below 100 indicates slower growth. This metric helps identify areas that are actively addressing housing shortages.\n\n\n\nConclusion\n\nBy focusing on metropolitan areas like Columbus, GA-AL and Fresno, CA, which have demonstrated success in reducing rent burdens while experiencing population growth, we can develop effective strategies to address housing affordability across the United States. Investing in affordable housing initiatives will not only improve the quality of life for residents but also contribute to the overall economic health of these metropolitan areas. We urge policymakers to consider these metrics and examples when formulating housing policies to ensure that all residents have access to safe and affordable housing options."
  },
  {
    "objectID": "mp02.html#initial-visualizations",
    "href": "mp02.html#initial-visualizations",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Initial Visualizations",
    "text": "Initial Visualizations\nThe relationship between monthly rent and average household income per CBSA in 2009.\n\n\nCode\np1 &lt;- ggplot(combined_data |&gt; filter(year == 2009), \n             aes(x = household_income, y = monthly_rent)) +\n    geom_point(alpha = 0.6, color=\"blue\") +\n    geom_smooth(method = \"lm\", color = \"#F7B800\") +\n    labs(title = \"Relationship between Monthly Rent and Average Household Income (2009)\",\n         x = \"Average Household Income\",\n         y = \"Monthly Rent\",\n        caption = \"Source: US Census Bureau ACS\") +\n    theme_grey( base_size = 18) + \n    scale_x_continuous(labels = scales::dollar_format()) +\n    scale_y_continuous(labels = scales::dollar_format()) +\n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\nprint(p1)\n\n\n\n\n\n\n\n\n\n\nThe scatter plot shows a strong positive relationship between household income and monthly rent in 2009. Metro areas tend to have higher average household incomes which results in higher monthly rents. The linear regression line indicates that as household income increases, monthly rent also tends to increase.\n\nThe relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs. Design your visualization so that it is possible to see the evolution of this relationship over time.\n\n\nCode\n# get the total employment per year per cbsa\nemp_total &lt;- WAGES |&gt; \n    group_by(YEAR) |&gt; \n  summarise(total_employment = sum(EMPLOYMENT)) |&gt; \n    ungroup()\n\n# get total employment of health\nhealth_total &lt;- WAGES |&gt; \n    filter(str_starts(as.character(INDUSTRY), \"62\")) |&gt; \n    group_by(YEAR) |&gt; \n    summarise(health_employment = sum(EMPLOYMENT)) |&gt;\n    ungroup()\n# join both and calculate fraction\ngrouped_emp &lt;- inner_join(emp_total, health_total, by = \"YEAR\") |&gt; \n    mutate(fraction_health_employment = health_employment / total_employment)\n\np2 &lt;- ggplot(grouped_emp, aes(x= YEAR, y =fraction_health_employment)) +\n    geom_line(size = 0.8, color = \"#F7B800\") +\n    geom_point(color = \"blue\") +\n    labs(title = \"Total and Health Employment Percantage over time\",\n         x = \"Year\",\n         y = \"Percentage of Health Employment to Total Employment\",\n         caption = \"Source: BLS QCEW\") +\n    theme_grey(base_size = 18) + \n    scale_y_continuous(labels = scales::percent_format(accuarcy = 1)) +\n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\nprint(p2)\n\n\n\n\n\n\n\n\n\n\nThe plot shows an increase of percentage of Health Employees to Total Employees over time. This indicates that the health care and social services sector has been growing in the economy. There was noticeable growth from 2019 to 2021 due to the pandemic.\n\nThe evolution of average household size over time. Use different lines to represent different CBSAs.\n\n\nCode\np3 &lt;- ggplot(combined_data, aes(x= year, y = population / households, \n                                group = GEOID, \n                                color =case_when(GEOID == 35620 ~ \"New York\",\n                                                 GEOID == 31080 ~ \"Los Angeles\"))) +\n    geom_line(linewidth = 0.8) +\n   scale_color_manual(values = c(\"New York\" = \"#F7B800\", \"Los Angeles\" = \"green\"),\n                     name = \"City\") +\n  gghighlight(GEOID %in% c(35620, 31080), \n              use_direct_label = FALSE, \n              unhighlighted_params = list(alpha=0.5)) +\n    labs(title = \"Average Household Size over Time by CBSA\",\n         x = \"Year\",\n         y = \"Average Household Size\",\n         caption = \"Source: US Census Bureau ACS\") +\n    theme_grey(base_size = 18) + \n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\nprint(p3)\n\n\n\n\n\n\n\n\n\n\nThe plot shows the average household size over time for different CBSAs, with a focus on New York and Los Angeles. Both cities show a slight decrease in average household size over the years, indicating a trend towards smaller households in these metropolitan areas."
  }
]
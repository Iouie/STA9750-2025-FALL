[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stanley Louie",
    "section": "",
    "text": "Hello, my name is Stanley and I’m currently a masters student at Baruch studying Statistics. I’m expected to graduate on December 2027.\n\nEducation\nCUNY Baruch New York | M.S in Statistics | Aug 2025 - Present\nRochester Institute of Technology | B.S in New Media Interactive Development | Aug 2016 - May 2022\n\n\nProjects\n\n\n\nMini Project 01\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. In venenatis.\n\n\n\n\nMini Project 02\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus at.\n\n\n\n\nMini Project 03\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus at.\n\n\n\n\nMini Project 04\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus at.\n\n\n\n\nFinal Project\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus at.\n\n\n\n\nLast Updated: Tuesday 09 30, 2025 at 18:20PM"
  },
  {
    "objectID": "docs/mp01.html",
    "href": "docs/mp01.html",
    "title": "NETFLIX",
    "section": "",
    "text": "Last Updated: Sunday 09 28, 2025 at 17:48PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Netflix’s Most Popular Programs: An Indepth Analysis",
    "section": "",
    "text": "1) SUMMARY\nNetflix has been investing heavily in original TV series and film content with the main purpose of attracting a worldwide audience. Netflix is trying to put out a series of press releases highlighting their recent successes. The data analyzes the most popular Netflix films and TV shows between 2021 and 2025 in countries where Netflix is available and globally throughout the world.\n\n\n2) DATA EXPLORATION\nWe are gathering the data from Netflix’s TumDum Top 10, and cleaning it.\n\n\nCode\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\ninstall.packages(c(\"ggplot2\", \"dplyr\", \"readr\", \"rnaturalearth\", \"rnaturalearthdata\"))\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(ggplot2)\nlibrary(scales)\n\n\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\n# Data cleaning to replace \"N/A\" string with N/A\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME, na=c(\"N/A\"))\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na=c(\"N/A\"))\n\n\n\n\n3) FUN FACTS\n\nNetflix operates in 94 different countries.\n\n\n\nCode\nnum_countries &lt;- COUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% count()\n\n\n\nOf all the non-english Netflix films, All Quiet on the Western Front ranks number one with 23 consecutive weeks in the global top 10.\n\n\n\nCode\nnon_engtop10 &lt;- GLOBAL_TOP_10 |&gt; select(category, show_title, cumulative_weeks_in_top_10) |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  arrange(desc(cumulative_weeks_in_top_10))\n\n\n\nThe longest film to have ever appeared in Netflix global Top 10 is Pushpa P: The Rule (Reloaded Version) which has a runtime of 224 minutes.\n\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; select(show_title, category, runtime) |&gt; \n  filter(category == \"Films (Non-English)\" | category == \"Films (English)\")|&gt; \n  mutate(runtime_mins = round(runtime * 60)) |&gt; arrange(desc(runtime_mins)) |&gt; \n  distinct(show_title, category, runtime, runtime_mins) \n\nlongest_film |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(longest_film),\n              color ='white')\n\n\n\n\n\n\n\nOf the four categories of Netflix’s Global Top 10, TV (English) has the most total hours of global viewership with a staggering 66,060,030,000 weekly hour views.\n\n\n\nCode\ntotal_weekly_viewers &lt;- GLOBAL_TOP_10 %&gt;% group_by(category) %&gt;% \n  summarise(total = sum(weekly_hours_viewed)) %&gt;% arrange(desc(total))\n\n\n\nMoney Heist has the longest run in a country’s Top 10 with 127 consecutive weeks in Pakistan.\n\n\n\nCode\ntv_tops &lt;- COUNTRY_TOP_10 |&gt; \n  select(category, show_title, cumulative_weeks_in_top_10,country_name) |&gt; \n  filter(category == \"TV\") |&gt; distinct(show_title, country_name, .keep_all = TRUE) |&gt;\n  arrange(desc(cumulative_weeks_in_top_10))\n\ntv_tops |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(tv_tops),\n              color ='white')\n\n\n\n\n\n\n\nThroughout 2021 to 2025, only Russia has less than 200 weeks of service with only 35 weeks of data.\n\n\n\nCode\ncountry_removed &lt;- COUNTRY_TOP_10 |&gt; group_by(country_name) |&gt; \n  summarise(num_weeks = n_distinct(week)) |&gt; \n  arrange(num_weeks)\n\ncountry_removed |&gt; \n    head(n=3) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(country_removed),\n              color ='white')\n\n\n\n\n\n\n\nThe total viewership for the TV show Squid Game from Season 1 to Season 3 is 5,048,300,000 hours.\n\n\n\nCode\ntot_hrs_squgame &lt;- GLOBAL_TOP_10 |&gt; \n  select(season_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(season_title, \"Squid Game: Season\")) |&gt; \n  summarise(total_hrs = sum(weekly_hours_viewed))\n\n\n\nThe movie Red Notice has 201,390,863 weekly hour views in 2021.\n\n\n\nCode\nred_weekly_views &lt;- GLOBAL_TOP_10 |&gt; \n  select(week, show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(week, \"2021\"),show_title == \"Red Notice\") |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  \n#convert Red Notice film time from 1hr 58 mins to 1.97hrs\n\n  summarise(red_total_views = total_wkly_hrs/ 1.97)\n\n\n\nThere are 45 films that reached No.1 but did not originally debut with the most recent one being Unknown Number: The High School Catfish in the US.\n\n\n\nCode\n# Create a variable that stores only us films\nus_films &lt;- COUNTRY_TOP_10 |&gt; \n  select(country_iso2,week, show_title, weekly_rank, category) |&gt; \n  filter(country_iso2 == \"US\", category == \"Films\")\n \n# Get all the films that eventually hit No.1 and most recent one.\n\nus_tops &lt;- us_films %&gt;% group_by(show_title) %&gt;% \n  summarise(debut_rank = weekly_rank[which.min(week)], \n            first_hit_week = min(week[weekly_rank == 1]), \n            hit_no1 = any(weekly_rank == 1)) %&gt;% \n  filter(hit_no1, debut_rank != 1) %&gt;% \n  select(show_title, debut_rank, first_hit_week) |&gt; \n  arrange(desc(first_hit_week))\n\nus_tops |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(us_tops),\n              color ='white')\n\n\n\n\n\n\n\nThere are 7 TV shows tied that hit top 10 in the most countries during its debut week with 94 countries.\n\n\n\nCode\ndebut &lt;- COUNTRY_TOP_10 %&gt;% filter(category ==\"TV\") %&gt;% \n  group_by(show_title, country_name) %&gt;% \n  summarise(debut_week = min(week), \n            debut_rank = weekly_rank[which.min(week)], \n            .groups=\"drop\" ) %&gt;% \n  group_by(show_title) %&gt;% \n  summarise(countries_in_top_10 = n_distinct(country_name), \n            .groups= \"drop\") %&gt;% \n  arrange(desc(countries_in_top_10))\n\n\ndebut |&gt; \n    head(n=10) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(debut),\n              color ='white')\n\n\n\n\n\n\n\n\n4) ENDGAME IN THE UPSIDE DOWN: STRANGER THINGS 5 BREAKS NEW GROUND\n\n\n\nStranger Things 5\n\n\nNetflix’s cultural phenomenon, Stranger Things, will unveil their fifth and final season on November 26. Since its debut in 2016, Stranger Things has captivated and garnered a worldwide audience, jump starting the trajectory of many Netflix original series that we know and love today. From the Upside Down to the Overworld, there will be a world where Stranger Things will reclaim its throne as Netflix’s most popular TV show.\nThroughout it’s 4 seasons, Stranger Things has amassed 423,997,143 daily viewers.\n\n\nCode\nGLOBAL_TOP_10 |&gt; select(show_title, weekly_hours_viewed) |&gt; filter(show_title ==\n                                                                     'Stranger Things') |&gt;\n  group_by(show_title) |&gt; summarise(total_weekly_hrs_stranger_things = sum(weekly_hours_viewed)) |&gt;\n  mutate(daily_hours_viewed =\n           total_weekly_hrs_stranger_things / 7)\n\n\n# A tibble: 1 × 3\n  show_title      total_weekly_hrs_stranger_things daily_hours_viewed\n  &lt;chr&gt;                                      &lt;dbl&gt;              &lt;dbl&gt;\n1 Stranger Things                       2967980000         423997143.\n\n\nOut of the 94 countries that Netflix operates in, Stranger Things have been in the top 10 for 93 of them with an average of 13 consecutive weeks in the top 10.\n\n\nCode\nnum_countries_top10 &lt;- COUNTRY_TOP_10 |&gt; \n  select(country_name, show_title, cumulative_weeks_in_top_10) |&gt; \n  filter(show_title =='Stranger Things') |&gt; \n  group_by(country_name) |&gt; \n  summarise(num_countries = n_distinct(cumulative_weeks_in_top_10))\n\nweekly_consecutive_top10_avg &lt;- sum(num_countries_top10$num_countries) / 93\n\nnum_countries_top10 |&gt; \n    datatable(options=list(paging=TRUE, searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(num_countries_top10),\n              color ='white')\n\n\n\n\n\n\nComparing Stranger Things to the 7 TV shows that hit top 10 during its debut week in 94 different countries, Stranger Things ranks as the 2nd highest total weekly hours only behind the cultural phenomenon Squid Game. Nearly a decade after its debut, Stranger Things continue to prove its staying power and relevance in the ever changing landscape of TV shows. As the series heads toward its conclusion, its continued dominance among the most-watched shows reaffirm its legacy as one of Netflix’s defining franchises.\n\n\nCode\nget_7_of_top10_debuts &lt;- GLOBAL_TOP_10 |&gt; \n  select(show_title, weekly_hours_viewed) |&gt; \n  filter(show_title == \"Stranger Things\" | \n           show_title == \"All of Us Are Dead\" | \n           show_title == \"Emily in Paris\"| \n           show_title ==\"Inventing Anna\" | \n           show_title == \"Sex Education\" | \n           show_title == \"Squid Game\" | \n           show_title ==\"The Witcher\" | \n           show_title == \"You\") |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_weekly_hours = sum(weekly_hours_viewed))\n\nggplot(get_7_of_top10_debuts, aes(x = reorder(show_title, -total_weekly_hours), \n                                  y = total_weekly_hours)) + \n  geom_col(fill = \"#375b7f\") + \n  labs(title = \"Total Weekly Hour Views\",x = \"Show Title\", \n       y = \"Total Hours Viewed\") + theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))\n\n\n\n\n\n\n\n\n\n\n\n5) FROM BOLLYWOOD TO BLOCKBUSTERS: INDIA IS ON THE RISE\n\n\n\nNetflix India\n\n\nIndia is starting to become one of the big players in the market. Hindi films have started to make strides and hitting top weekly charts. The Hindi film RRR (Hindi) has been in the top 10 cumulatively for 18 weeks. The show has a weekly hour view of 79,780,000. Don’t sleep on the other films either! All Hindi films that have appeared in Netflix’s top 10 total up to 183,000,000 weekly hours viewed.\n\n\n\nCode\ntop_hindi_films &lt;- GLOBAL_TOP_10 |&gt; select(show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(show_title, \"Hindi\")) |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  slice_max(total_wkly_hrs, n = 5)\n\nggplot(top_hindi_films, aes(x= reorder(show_title, total_wkly_hrs), y=total_wkly_hrs)) +\n  geom_col(fill = \"#375b7f\") +\n  coord_flip() +\n  labs(x =\"Hindi Shows\", y = \"Weekly Hours Viewed\", title = \"Top Hindi Shows\")+\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nhighest_cum_wks &lt;- GLOBAL_TOP_10 %&gt;% \n  filter(str_detect(show_title, \"Hindi\")) |&gt; \n  group_by(show_title) |&gt; \n  slice_max(cumulative_weeks_in_top_10, n=1) %&gt;% ungroup() %&gt;%\n  arrange(desc(cumulative_weeks_in_top_10)) %&gt;%\n  slice_head(n = 5)\n\nggplot(highest_cum_wks, aes(x= reorder(show_title, -cumulative_weeks_in_top_10), \n                            y=cumulative_weeks_in_top_10)) +\n  geom_col(fill = \"#375b7f\") +\n  labs(x =\"Hindi Shows\", y = \"Cumulative Weeks in Top 10\", title = \"Top Hindi Shows\")\n\n\n\n\n\n\n\n\n\n\n\n6) WWE AND NETFLIX TAG TEAM TO BRING WRESTLING FANS UNMATCHED ACCESS\n\n\n\nNetflix WWE\n\n\nFor over 70 years, WWE has been the heart and soul of sports entertainment, spanning across generations, developing iconic superstars, and generating legendary and unforgettable moments watched by millions across the globe. On January 6, 2025, WWE will officially be streamed live on Netflix. WWE fans will now be able to stream shows at anywhere, anytime, and on any device. Thousands of new audiences have already flocked in to watch WWE’s special Pay-Per-Views resulting in WWE getting into the weekly top 10 in 42 different countries!\n\n\nCode\nwwe_tv &lt;- COUNTRY_TOP_10 %&gt;% filter(str_detect(show_title, \"WWE\"))\n\n#get count of unique countries\nwwe_tv %&gt;% summarise(unique_count = n_distinct(country_name))\n\n\n# A tibble: 1 × 1\n  unique_count\n         &lt;int&gt;\n1           42\n\n\nCode\nworld_map &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nworld_map &lt;- world_map %&gt;%\n  mutate(highlight = admin %in% wwe_tv$country_name)\n\nggplot(world_map) +\n  geom_sf(aes(fill = highlight), color = \"grey80\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"black\")) +\n  theme_minimal() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n In the map above, we can see that WWE has a strong presence in North America, South America, Europe, and parts of Asia. However, there are still many countries in Africa and the Middle East that have yet to embrace the thrill of WWE. With Netflix’s global reach, WWE has the potential to expand its audience and bring the excitement of wrestling to new regions. Speaking of regions, the graph below shows the number of times a WWE show has hit the weekly top 10. It’s not surprising that the 2 biggest events, WrestleMania and SummerSlam, are the most popular show.\n\n\nCode\ncountry_count &lt;- wwe_tv %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(num_countries = n_distinct(country_name))\n\nggplot(country_count, aes(x= reorder(show_title, num_countries), y=num_countries)) +\n  geom_col(fill = \"#375b7f\") +\n  coord_flip() +\n  labs(x =\"WWE Shows\", y = \"# of Countries that WWE Hit Top 10\", title = \"WWE Popularity\")\n\n\n\n\n\n\n\n\n\n\n\nLast Updated: Tuesday 10 21, 2025 at 21:37PM"
  },
  {
    "objectID": "mp01.html#acquiring-data",
    "href": "mp01.html#acquiring-data",
    "title": "Netflix’s Most Popular Programs: An Indepth Analysis",
    "section": "",
    "text": "if(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n----------\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(readr)\nlibrary(dplyr)\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\n---------------------\n\n\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt; mutate(season_title = if_else(season_title ==\"N/A\", NA, season_title))\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na=c(\"N/A\"))\n\n1) How many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n### create new variable to store the unique countries column and their initials. Total: 94\n\nall_countries &lt;- distinct(COUNTRY_TOP_10, country_name, country_iso2)  \n\n2) Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n### create new variable to show noneng top 10 shows, only filter category to nonenglish films, select category, show title, and cumulative weeks\n\nnon_engtop10 &lt;- GLOBAL_TOP_10 |&gt; filter(category == \"Films (Non-English)\") |&gt; select(category, show_title, cumulative_weeks_in_top_10) |&gt; arrange(desc(cumulative_weeks_in_top_10)). All Quiet on the Western Front | 23 weeks\n\n3) What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\nNote that Netflix does not provide runtime for programs before a certain date, so your answer here may be a bit limited.\n\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; select(show_title, category, runtime) |&gt; filter(category == \"Films (Non-English)\" | category == \"Films (English)\")|&gt; mutate(runtime_mins = round(runtime * 60)) |&gt; arrange(desc(runtime_mins)) |&gt; distinct(show_title, category, runtime, runtime_mins) \n\nPushpa 2: The Rule (Reloaded Version) Runtime: 224mins\n\n4) For each of the four categories, what program has the most total hours of global viewership?\n\ntotal_weekly_viewers&lt;- GLOBAL_TOP_10 %&gt;% group_by(category) %&gt;% summarise(total = sum(weekly_hours_viewed)) %&gt;% arrange(desc(total))\n\n5) Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\ntv_tops &lt;- COUNTRY_TOP_10 |&gt; select(category, show_title, cumulative_weeks_in_top_10,country_name) |&gt; filter(category == \"TV\") |&gt; distinct(show_title, country_name, .keep_all = TRUE) |&gt; arrange(desc(cumulative_weeks_in_top_10))\n\nMoney Heist: Pakistan 127 consecutive weeks\n\n7) What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\ntot_hrs_squgame &lt;- GLOBAL_TOP_10 |&gt; select(season_title, weekly_hours_viewed) |&gt; filter(str_detect(season_title, \"Squid Game: Season\")) |&gt; summarise(total_hrs = sum(weekly_hours_viewed))\n\n8) The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\nred_weekly_views &lt;- GLOBAL_TOP_10 |&gt; select(week, show_title, weekly_hours_viewed) |&gt; filter(str_detect(week, \"2021\"),show_title == \"Red Notice\") |&gt; summarise(total_wkly_hrs = sum(weekly_hours_viewed)) \n\n396740000\n\ntotal weekly hours viewed / 1.97hrs\n\nred_weekly_views &lt;- red_weekly_views / 1.97 = 201390863 total views in 2021\n\n9) How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\nus_films &lt;- COUNTRY_TOP_10 |&gt; select(country_iso2,week, show_title, weekly_rank, category) |&gt; filter(country_iso2 == \"US\", category == \"Films\") SHOW US FILMS ONLY\n\nus_tops &lt;- us_films %&gt;% group_by(show_title) %&gt;% summarise(debut_rank = weekly_rank[which.min(week)], first_hit_week = min(week[weekly_rank == 1]), hit_no1 = any(weekly_rank == 1)) %&gt;% filter(hit_no1, debut_rank != 1) %&gt;% select(show_title, debut_rank, first_hit_week) |&gt; arrange(desc(first_hit_week))\n\n10) Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\ndebut &lt;- COUNTRY_TOP_10 %&gt;% filter(category ==\"TV\") %&gt;% group_by(show_title, country_name) %&gt;% summarise(debut_week = min(week), debut_rank = weekly_rank[which.min(week)], .groups=\"drop\" ) %&gt;% group_by(show_title) %&gt;% summarise(countries_in_top_10 = n_distinct(country_name), .groups= \"drop\") %&gt;% arrange(desc(countries_in_top_10))\n\n\nLast Updated: Monday 09 29, 2025 at 21:58PM"
  },
  {
    "objectID": "mp01.html#fun-facts",
    "href": "mp01.html#fun-facts",
    "title": "Netflix’s Most Popular Programs: An Indepth Analysis",
    "section": "1.1) FUN FACTS",
    "text": "1.1) FUN FACTS\nWe are gathering the data from Netflix’s TumDum Top 10 which consists of\n\nGlobal Top 10\nCountry-wide Top 10\n\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\ninstall.packages(c(\"ggplot2\", \"dplyr\", \"readr\", \"rnaturalearth\", \"rnaturalearthdata\"))\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(ggplot2)\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\n# Data cleaning to replace \"N/A\" string with N/A\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME, na=c(\"N/A\"))\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na=c(\"N/A\"))\n\n\n\nNetflix operates in 94 different countries.\n\n\n\nCode\nnum_countries &lt;- COUNTRY_TOP_10 %&gt;% distinct(country_name) %&gt;% count()\n\n\n\nOf all the non-english Netflix films, All Quiet on the Western Front spent 23 consecutive weeks in the global top 10.\n\n\n\nCode\nnon_engtop10 &lt;- GLOBAL_TOP_10 |&gt; select(category, show_title, cumulative_weeks_in_top_10) |&gt; \n  filter(category == \"Films (Non-English)\") |&gt; \n  arrange(desc(cumulative_weeks_in_top_10))\n\n\n\nThe longest film to have ever appeared in Netflix global Top 10 is Pushpa P: The Rule (Reloaded Version) which lasts for 224 minutes.\n\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; select(show_title, category, runtime) |&gt; \n  filter(category == \"Films (Non-English)\" | category == \"Films (English)\")|&gt; \n  mutate(runtime_mins = round(runtime * 60)) |&gt; arrange(desc(runtime_mins)) |&gt; \n  distinct(show_title, category, runtime, runtime_mins) \n\nlongest_film |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(longest_film),\n              color ='white')\n\n\n\n\n\n\n\nOf the four categories of Netflix’s Global Top 10, TV (English) has the most total hours of global viewership with a staggering 66,060,030,000 weekly hours viewed since 2021.\n\n\n\nCode\ntotal_weekly_viewers &lt;- GLOBAL_TOP_10 %&gt;% group_by(category) %&gt;% \n  summarise(total = sum(weekly_hours_viewed)) %&gt;% arrange(desc(total))\n\n\n\nMoney Heist had the longest run in a country’s Top 10 with 127 consecutive weeks in the top 10 in Pakistan.\n\n\n\nCode\ntv_tops &lt;- COUNTRY_TOP_10 |&gt; \n  select(category, show_title, cumulative_weeks_in_top_10,country_name) |&gt; \n  filter(category == \"TV\") |&gt; distinct(show_title, country_name, .keep_all = TRUE) |&gt;\n  arrange(desc(cumulative_weeks_in_top_10))\n\ntv_tops |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(tv_tops),\n              color ='white')\n\n\n\n\n\n\n\nThroughout 2021 to 2025, only Russia has less than 200 weeks of service with only 35 weeks.\n\n\n\nCode\ncountry_removed &lt;- COUNTRY_TOP_10 |&gt; group_by(country_name) |&gt; \n  summarise(num_weeks = n_distinct(week)) |&gt; \n  arrange(num_weeks)\n\ncountry_removed |&gt; \n    head(n=3) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(country_removed),\n              color ='white')\n\n\n\n\n\n\n\nThe total viewership for the TV show Squid Game from Season 1 to Season 3 is 5,048,300,000 hours.\n\n\n\nCode\ntot_hrs_squgame &lt;- GLOBAL_TOP_10 |&gt; \n  select(season_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(season_title, \"Squid Game: Season\")) |&gt; \n  summarise(total_hrs = sum(weekly_hours_viewed))\n\n\n\nThe movie Red Notice has 201,390,863 weekly hour views in 2021.\n\n\n\nCode\nred_weekly_views &lt;- GLOBAL_TOP_10 |&gt; \n  select(week, show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(week, \"2021\"),show_title == \"Red Notice\") |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  \n#convert Red Notice film time from 1hr 58 mins to 1.97hrs\n\n  summarise(red_total_views = total_wkly_hrs/ 1.97)\n\n\n\nThere are 45 films that reached No.1 but did not originally debut with the most recent one being Unknown Number: The High School Catfish in the US.\n\n\n\nCode\n# Create a variable that stores only us films\nus_films &lt;- COUNTRY_TOP_10 |&gt; \n  select(country_iso2,week, show_title, weekly_rank, category) |&gt; \n  filter(country_iso2 == \"US\", category == \"Films\")\n \n# Get all the films that eventually hit No.1 and most recent one.\n\nus_tops &lt;- us_films %&gt;% group_by(show_title) %&gt;% \n  summarise(debut_rank = weekly_rank[which.min(week)], \n            first_hit_week = min(week[weekly_rank == 1]), \n            hit_no1 = any(weekly_rank == 1)) %&gt;% \n  filter(hit_no1, debut_rank != 1) %&gt;% \n  select(show_title, debut_rank, first_hit_week) |&gt; \n  arrange(desc(first_hit_week))\n\nus_tops |&gt; \n    head(n=5) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(us_tops),\n              color ='white')\n\n\n\n\n\n\n\nThere are 7 TV shows tied that hit top 10 in the most countries during its debut week with 94 countries.\n\n\n\nCode\ndebut &lt;- COUNTRY_TOP_10 %&gt;% filter(category ==\"TV\") %&gt;% \n  group_by(show_title, country_name) %&gt;% \n  summarise(debut_week = min(week), \n            debut_rank = weekly_rank[which.min(week)], \n            .groups=\"drop\" ) %&gt;% \n  group_by(show_title) %&gt;% \n  summarise(countries_in_top_10 = n_distinct(country_name), \n            .groups= \"drop\") %&gt;% \n  arrange(desc(countries_in_top_10))\n\n\ndebut |&gt; \n    head(n=10) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt; \n  formatStyle(columns = names(debut),\n              color ='white')"
  },
  {
    "objectID": "mp01.html#gathering-data",
    "href": "mp01.html#gathering-data",
    "title": "Netflix’s Most Popular Programs: An Indepth Analysis",
    "section": "",
    "text": "We are gathering the data from Netflix’s TumDum Top 10 which consists of\n\nGlobal Top 10\nCountry-wide Top 10\n\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\nlibrary(readr)\nlibrary(dplyr)\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na=c(\"N/A\"))\n\n\n\nHow many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n\n\nall_countries &lt;- distinct(COUNTRY_TOP_10, country_name, country_iso2)\n\nWhich non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\n\n\nnon_engtop10 &lt;- GLOBAL_TOP_10 |&gt; filter(category == “Films (Non-English)”) |&gt; select(category, show_title, cumulative_weeks_in_top_10) |&gt; arrange(desc(cumulative_weeks_in_top_10)). All Quiet on the Western Front | 23 weeks\n\nWhat is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\nNote that Netflix does not provide runtime for programs before a certain date, so your answer here may be a bit limited.\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; select(show_title, category, runtime) |&gt; filter(category == “Films (Non-English)” | category == “Films (English)”)|&gt; mutate(runtime_mins = round(runtime * 60)) |&gt; arrange(desc(runtime_mins)) |&gt; distinct(show_title, category, runtime, runtime_mins)\nPushpa 2: The Rule (Reloaded Version) Runtime: 224mins\n\nFor each of the four categories, what program has the most total hours of global viewership?\n\ntotal_weekly_viewers&lt;- GLOBAL_TOP_10 %&gt;% group_by(category) %&gt;% summarise(total = sum(weekly_hours_viewed)) %&gt;% arrange(desc(total))\n\nWhich TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\ntv_tops &lt;- COUNTRY_TOP_10 |&gt; select(category, show_title, cumulative_weeks_in_top_10,country_name) |&gt; filter(category == “TV”) |&gt; distinct(show_title, country_name, .keep_all = TRUE) |&gt; arrange(desc(cumulative_weeks_in_top_10))\nMoney Heist: Pakistan 127 consecutive weeks\n\nWhat is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\ntot_hrs_squgame &lt;- GLOBAL_TOP_10 |&gt; select(season_title, weekly_hours_viewed) |&gt; filter(str_detect(season_title, “Squid Game: Season”)) |&gt; summarise(total_hrs = sum(weekly_hours_viewed))\n\nThe movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\nred_weekly_views &lt;- GLOBAL_TOP_10 |&gt; select(week, show_title, weekly_hours_viewed) |&gt; filter(str_detect(week, “2021”),show_title == “Red Notice”) |&gt; summarise(total_wkly_hrs = sum(weekly_hours_viewed))\n396740000\ntotal weekly hours viewed / 1.97hrs\nred_weekly_views &lt;- red_weekly_views / 1.97 = 201390863 total views in 2021\n\nHow many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\nus_films &lt;- COUNTRY_TOP_10 |&gt; select(country_iso2,week, show_title, weekly_rank, category) |&gt; filter(country_iso2 == “US”, category == “Films”) SHOW US FILMS ONLY\nus_tops &lt;- us_films %&gt;% group_by(show_title) %&gt;% summarise(debut_rank = weekly_rank[which.min(week)], first_hit_week = min(week[weekly_rank == 1]), hit_no1 = any(weekly_rank == 1)) %&gt;% filter(hit_no1, debut_rank != 1) %&gt;% select(show_title, debut_rank, first_hit_week) |&gt; arrange(desc(first_hit_week))\n\nWhich TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\ndebut &lt;- COUNTRY_TOP_10 %&gt;% filter(category ==“TV”) %&gt;% group_by(show_title, country_name) %&gt;% summarise(debut_week = min(week), debut_rank = weekly_rank[which.min(week)], .groups=“drop” ) %&gt;% group_by(show_title) %&gt;% summarise(countries_in_top_10 = n_distinct(country_name), .groups= “drop”) %&gt;% arrange(desc(countries_in_top_10))\n```\n\nLast Updated: Monday 09 29, 2025 at 22:29PM"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "",
    "text": "Housing affordability is an increasing challenge to many metropolitan areas. Rent growth has begun to outpace income, causing financial strain on many households. In this mini-project, we will explore data from the US Census Bureau and the Bureau of Labor Statistics to analyze housing affordability trends across different metropolitan areas in the United States. We will compute rent burden metrics, housing growth metrics, and identify metropolitan areas that have successfully managed to provide affordable housing options for their residents."
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "",
    "text": "Housing affordability is an increasing challenge to many metropolitan areas. Rent growth has begun to outpace income, causing financial strain on many households. In this mini-project, we will explore data from the US Census Bureau and the Bureau of Labor Statistics to analyze housing affordability trends across different metropolitan areas in the United States. We will compute rent burden metrics, housing growth metrics, and identify metropolitan areas that have successfully managed to provide affordable housing options for their residents."
  },
  {
    "objectID": "mp02.html#data-acquisition-and-preparation",
    "href": "mp02.html#data-acquisition-and-preparation",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Data Acquisition and Preparation",
    "text": "Data Acquisition and Preparation\nGrabbing data from US Census Bureau.\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(gghighlight)\nlibrary(scales)\nlibrary(DT)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\nManually grabbing number of new housing units built each year\n\n\nCode\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\n\nBLS data NAICS coding system\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    \n    if(!file.exists(fname)){\n    \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code)\n    \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n    \n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\nBLS QUARTERLY CENSUS\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\nJOINS household, household income, population, rent\n\n\nCode\ncombined_data &lt;- inner_join(HOUSEHOLDS, INCOME, by=c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n    inner_join(POPULATION, by=c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n    inner_join(RENT, by=c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n    inner_join(PERMITS, by=c(\"GEOID\" = \"CBSA\", \"year\")) |&gt;\n    mutate(std_cbsa = paste0(\"C\", GEOID))\n\nWAGES &lt;- WAGES |&gt;\n    mutate(std_cbsa = paste0(FIPS, \"0\"))"
  },
  {
    "objectID": "mp02.html#exploratory-analysis",
    "href": "mp02.html#exploratory-analysis",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nWhich CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\nlargest_units &lt;- combined_data |&gt;\n    filter(year &gt;= 2010, year &lt;= 2019) |&gt;\n    group_by(GEOID) |&gt;\n    summarize(total_permitted = sum(new_housing_units_permitted, na.rm=TRUE)) |&gt;\n    arrange(desc(total_permitted)) |&gt;\n    slice_head(n=1) \n\n\n\nFrom 2010 to 2019, the CBSA that permitted the largest number of new housing units is\n26420 with a total of 482,075 units permitted.\n\nIn what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nabq_max_year &lt;- combined_data |&gt;\n    filter(GEOID == 10740) |&gt;\n    arrange(desc(new_housing_units_permitted)) |&gt;\n    slice_head(n=1)\n\n\n\nAlbuquerque, NM (CBSA Number 10740) permitted the most new housing units in the year 2021 with 4,021 units permitted.\n\nWhich state (not CBSA) had the highest average individual income in 2015? To answer this question, you will need to first compute the total income per CBSA by multiplying the average household income by the number of households, and then sum total income and total population across all CBSAs in a state. With these numbers, you can answer this question.\n\n\nCode\nstate_income_2015 &lt;- combined_data |&gt;\n    filter(year == 2015) |&gt;\n    mutate(total_income = household_income * households) |&gt;\n    mutate(state = str_extract(NAME, \", (.{2})\", group=1)) |&gt;\n    group_by(state) |&gt;\n    summarize(total_income = sum(total_income, na.rm=TRUE),\n              total_population = sum(population, na.rm=TRUE)) |&gt;\n    mutate(average_individual_income = total_income / total_population) |&gt;\n    arrange(desc(average_individual_income)) |&gt;\n    slice_head(n=1)\n\n\n\nThe state with the highest average individual income in 2015 is DC with an average individual income of $33,232.88.\n\nData scientists and business analysts are recorded under NAICS code 5182. What is the last year in which the NYC CBSA had the most data scientists in the country? In recent, the San Francisco CBSA has had the most data scientists. For this question, you may simply create a table of which CBSA had the most data scientists each year and then answer the question in the following text.\n\n\nCode\nareas &lt;- combined_data |&gt; select(NAME,std_cbsa)\n\ndata_sci &lt;- WAGES |&gt; \n    filter(INDUSTRY == 5182) |&gt; \n    group_by(YEAR,std_cbsa) |&gt; filter(EMPLOYMENT !=0) |&gt; \n    summarise(max_employment = max(EMPLOYMENT, na.rm = TRUE)) |&gt; \n    slice_max(order_by = max_employment)\n\n#join table to get the city\ndata_sci &lt;- inner_join(data_sci, areas, by = \"std_cbsa\") |&gt; distinct()\n\n\n\nThe last year in which the NYC CBSA had the most data scientists in the country was 2015.\n\nWhat fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nCode\n# get the sum of total wages in NYC cbsa per year\ntotal_wages_sum &lt;- WAGES |&gt; \n    filter(std_cbsa == \"C35620\") |&gt; group_by(YEAR) |&gt; summarise(sum_total_wages = sum(TOTAL_WAGES))\n\n# get the sum of financial wages in NYC cbsa per year\nfinancial_wages_sum &lt;- WAGES |&gt; \n    filter(std_cbsa == \"C35620\", str_starts(as.character(INDUSTRY), \"52\")) |&gt; \n    group_by(YEAR) |&gt; summarise(sum_financial_wages = sum(TOTAL_WAGES))\n\n# join both and calculate fraction\nwage_fraction &lt;- inner_join(total_wages_sum, financial_wages_sum, by = \"YEAR\") |&gt;\n    mutate(fraction_of_financial_wages = sum_financial_wages / sum_total_wages) |&gt; arrange(desc(fraction_of_financial_wages))\n\n# getting the total fraction\ntotal_fraction &lt;- wage_fraction |&gt; summarise(fraction_total = sum(sum_financial_wages) / sum(sum_total_wages) * 100)\n\n\n\nThe fraction of total wages in the NYC CBSA earned by people employed in the finance and insurance industries is 13.29%. This fraction peaked in the year 2021 with a fraction of 15.45%."
  },
  {
    "objectID": "mp02.html#final-insights-and-deliverable",
    "href": "mp02.html#final-insights-and-deliverable",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Final Insights and Deliverable",
    "text": "Final Insights and Deliverable\nCode related to the final deliverable of the assignment goes here.\n\nThis work ©2025 by iouie was initially prepared as a Mini-Project for STA 9750 at Baruch College. More details about this course can be found at the course site and instructions for this assignment can be found at MP #02"
  },
  {
    "objectID": "mp02.html#task-3",
    "href": "mp02.html#task-3",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Task 3",
    "text": "Task 3\nThe relationship between monthly rent and average household income per CBSA in 2009.\n\n\nCode\np1 &lt;- ggplot(combined_data |&gt; filter(year == 2009), \n             aes(x = household_income, y = monthly_rent)) +\n    geom_point(alpha = 0.6, color=\"blue\") +\n    geom_smooth(method = \"lm\", color = \"#F7B800\") +\n    labs(title = \"Relationship between Monthly Rent and Average Household Income (2009)\",\n         x = \"Average Household Income\",\n         y = \"Monthly Rent\",\n        caption = \"Source: US Census Bureau ACS\") +\n    theme_grey( base_size = 18) + \n    scale_x_continuous(labels = scales::dollar_format()) +\n    scale_y_continuous(labels = scales::dollar_format()) +\n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\nprint(p1)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThe scatter plot shows a strong positive relationship between household income and monthly rent in 2009. Metro areas tend to have higher average household incomes which results in higher monthly rents. The linear regression line indicates that as household income increases, monthly rent also tends to increase.\n\nThe relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs. Design your visualization so that it is possible to see the evolution of this relationship over time.\n\n\nCode\n# get the total employment per year per cbsa\nemp_total &lt;- WAGES |&gt; \n    group_by(YEAR) |&gt; \n  summarise(total_employment = sum(EMPLOYMENT)) |&gt; \n    ungroup()\n\n# get total employment of health\nhealth_total &lt;- WAGES |&gt; \n    filter(str_starts(as.character(INDUSTRY), \"62\")) |&gt; \n    group_by(YEAR) |&gt; \n    summarise(health_employment = sum(EMPLOYMENT)) |&gt;\n    ungroup()\n# join both and calculate fraction\ngrouped_emp &lt;- inner_join(emp_total, health_total, by = \"YEAR\") |&gt; \n    mutate(fraction_health_employment = health_employment / total_employment)\n\np2 &lt;- ggplot(grouped_emp, aes(x= YEAR, y =fraction_health_employment)) +\n    geom_line(size = 0.8, color = \"#F7B800\") +\n    geom_point(color = \"blue\") +\n    labs(title = \"Total and Health Employment Percantage over time\",\n         x = \"Year\",\n         y = \"Percentage of Health Employment to Total Employment\",\n         caption = \"Source: BLS QCEW\") +\n    theme_grey(base_size = 18) + \n    scale_y_continuous(labels = scales::percent_format(accuarcy = 1)) +\n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nCode\nprint(p2)\n\n\n\n\n\n\n\n\n\n\nThe plot shows an increase of percentage of Health Employees to Total Employees over time. This indicates that the health care and social services sector has been growing in the economy. There was noticeable growth from 2019 to 2021 due to the pandemic.\n\nThe evolution of average household size over time. Use different lines to represent different CBSAs.\n\n\nCode\np3 &lt;- ggplot(combined_data, aes(x= year, y = population / households, \n                                group = GEOID, \n                                color =case_when(GEOID == 35620 ~ \"New York\",\n                                                 GEOID == 31080 ~ \"Los Angeles\"))) +\n    geom_line(linewidth = 0.8) +\n   scale_color_manual(values = c(\"New York\" = \"#F7B800\", \"Los Angeles\" = \"green\"),\n                     name = \"City\") +\n  gghighlight(GEOID %in% c(35620, 31080), \n              use_direct_label = FALSE, \n              unhighlighted_params = list(alpha=0.5)) +\n    labs(title = \"Average Household Size over Time by CBSA\",\n         x = \"Year\",\n         y = \"Average Household Size\",\n         caption = \"Source: US Census Bureau ACS\") +\n    theme_grey(base_size = 18) + \n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nCode\nprint(p3)\n\n\n\n\n\n\n\n\n\n\nThe plot shows the average household size over time for different CBSAs, with a focus on New York and Los Angeles. Both cities show a slight decrease in average household size over the years, indicating a trend towards smaller households in these metropolitan areas."
  },
  {
    "objectID": "mp02.html#rent-burden",
    "href": "mp02.html#rent-burden",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Rent Burden",
    "text": "Rent Burden\n\n\nCode\nrent_burden_data &lt;- inner_join(INCOME, RENT, by=c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n    mutate(yearly_rent = monthly_rent * 12,\n    rent_burden = yearly_rent / household_income,\n    rent_burden_percent = yearly_rent/ household_income * 100)\n\n# scaling it\nlowest_burden &lt;- min(rent_burden_data$rent_burden_percent, na.rm = TRUE)\nhighest_burden &lt;- max(rent_burden_data$rent_burden_percent, na.rm = TRUE)\n\n# creating rbi\nrent_burden_data &lt;- rent_burden_data |&gt;\n    mutate(rbi = 100 * (rent_burden_percent - lowest_burden ) / (highest_burden - lowest_burden))\n\n\n\nRent burden is used to measure percentage of income spent on rent. I’ve standardized the metric so that 0 would be the lowest burden and 100 would be the highest in the observed data."
  },
  {
    "objectID": "mp02.html#metropolitan-california",
    "href": "mp02.html#metropolitan-california",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Metropolitan California",
    "text": "Metropolitan California\n\n\n\n\n\n\n\nLos Angeles has seen a small increase in rent burden from 2009 to 2023, with the Rent Burden Score rising from approximately 47.26 in 2009 to 52.91 in 2023 while drastically increasing in Household Income. The rent burden has been in the range from 24% to 26%."
  },
  {
    "objectID": "mp02.html#highest-and-lowest-metro-burden-areas",
    "href": "mp02.html#highest-and-lowest-metro-burden-areas",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Highest and Lowest Metro Burden Areas",
    "text": "Highest and Lowest Metro Burden Areas\n\n\nCode\nlow_burden &lt;- rent_burden_data |&gt;\n    filter(year == 2023, str_detect(NAME, \"Metro\")) |&gt;\n    select(\n      'City' = NAME, \n      'Year' = year, \n      'Rent Burden Percent' = rent_burden_percent, \n      'Rent Burden Score' = rbi) |&gt;\n    mutate(`Rent Burden Percent` = scales::percent(`Rent Burden Percent` / 100, accuracy = 0.1),\n           `Rent Burden Score` = round(`Rent Burden Score`, 2)) |&gt;\n    arrange(desc(`Rent Burden Score`)) |&gt; arrange(`Rent Burden Score`) |&gt;\n    head(10) \n    \ndatatable(\n  low_burden,\n  caption = \"Top 10 Lowest Rent Burden Metro Areas in 2023\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = FALSE)\n)\n\n\n\n\n\n\n\nThe lowest rent burden score is Bismarck, North Dakota with a 5.27 rent burden score with residents spending only 13.7% of their income on rent."
  },
  {
    "objectID": "mp02.html#lowest-metro-burden-areas",
    "href": "mp02.html#lowest-metro-burden-areas",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Lowest Metro Burden Areas",
    "text": "Lowest Metro Burden Areas\n\n\n\n\n\n\n\nThe lowest rent burden score is Bismarck, North Dakota with a 5.27 rent burden score with residents spending only 13.7% of their income on rent."
  },
  {
    "objectID": "mp02.html#housing-growth",
    "href": "mp02.html#housing-growth",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Housing Growth",
    "text": "Housing Growth\n\n\nCode\ntask5_table &lt;- POPULATION |&gt;\n  inner_join(PERMITS, by=c(\"year\", 'GEOID' = 'CBSA'))\n\n# Calculate 5-year growth using lag function\npopulation_growth_5yr &lt;- task5_table |&gt;\n  arrange(NAME, year) |&gt;\nmutate(\n  pop_5yrs_ago = lag(population, 5),\n  pop_growth_5yrs = population - lag(population, 5),\n  pop_growth_5yr_percent = (population - lag(population, 5)) / lag(population, 5) * 100,\n  permits_per_1000 = (new_housing_units_permitted / population) * 1000) |&gt;\n  ungroup()\n\n# Average permits per 1000 in 2014 as baseline\nhousing_growth &lt;- population_growth_5yr |&gt;\n  filter(year == 2014) |&gt;\n  summarise(baseline = mean(permits_per_1000, na.rm = TRUE)) |&gt;\n  pull(baseline)\n\n# Index where 100 = 2014 baseline\npopulation_growth_5yr &lt;- population_growth_5yr |&gt;\n  mutate(\n    instantaneous_score = (permits_per_1000 / housing_growth) * 100\n  )\n\npop_table &lt;- population_growth_5yr |&gt;\n  select(\n    'City' = NAME,\n    'Year' = year,\n    'Population (5 yrs Ago)' = pop_5yrs_ago,\n    'New Housing Units Permitted' = new_housing_units_permitted,\n    'Population Growth (5 yr)' = pop_growth_5yrs,\n    'Population Growth (5 yr %)' = pop_growth_5yr_percent,\n    'Permits per 1000 People' = permits_per_1000,\n    'Instantaneous Score' = instantaneous_score\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(\n    `Population Growth (5 yr %)` = scales::percent(`Population Growth (5 yr %)` / 100, accuracy = 0.1),\n    `Permits per 1000 People` = round(`Permits per 1000 People`, 2),\n    `Instantaneous Score` = round(`Instantaneous Score`, 2)\n  )\n\n\ndatatable(\n  pop_table,\n  caption = \"Instantaenous Scores\",\n  class = \"table table-striped table-hover table-dark table-sm\",\n  options = list(pageLength = 10,\n                 autoWidth = TRUE,\n                 searching = FALSE),\n)\n\n\n\n\n\n\n\nInstantaneous Scores above 100, Faster building housing than 2014 average\nInstantaneous Scores below 100, Slower building housing than 2014 average\nInstantaneous Scores 100, Housing building matches 2014 average"
  },
  {
    "objectID": "mp02.html#rate-base-housing",
    "href": "mp02.html#rate-base-housing",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Rate Base Housing",
    "text": "Rate Base Housing\n\n\n\n\n\n\n\nRate Scores above 100, Faster building housing than 2014 average population growth\nRate Scores below 100, Slower building housing than 2014 average population growth\nRate Scores 100, Housing building matches 2014 average population growth\nThere are many different states with extremely high Rate Scores. On the other hand, Connecticut, New York, and Ohio Metro areas have some of the lowest Rate Scores."
  },
  {
    "objectID": "mp02.html#composite-score",
    "href": "mp02.html#composite-score",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Composite Score",
    "text": "Composite Score\n\n\n\n\n\n\n\nSalisbury, Maryland has the highest Composite Score, while Morgantown, West Virginia has the lowest Composite Score."
  },
  {
    "objectID": "mp02.html#instantaneous-housing-growth",
    "href": "mp02.html#instantaneous-housing-growth",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Instantaneous Housing Growth",
    "text": "Instantaneous Housing Growth\n\n\n\n\n\n\n\nInstantaneous Scores above 100, Faster building housing than 2014 average\nInstantaneous Scores below 100, Slower building housing than 2014 average\nInstantaneous Scores 100, Housing building matches 2014 average\nFlorida and North/South Carolina Metro Areas have some of the highest Instantaneous Scores, occupying 9 of the top 10 spots, while Illinois, West Virginia, and Arkansas have some of the lowest Instataneous Scores."
  },
  {
    "objectID": "mp02.html#task-6-visualization",
    "href": "mp02.html#task-6-visualization",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Task 6: Visualization",
    "text": "Task 6: Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis scatter plot visualizes the relationship between early rent burden in 2014 and the change in rent burden up to 2023 for various CBSAs. The dashed lines indicate the median early rent burden and zero change in rent burden. CBSAs in the bottom-right show a desirable YIMBY characteristic of high early rent burden with a decrease over time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis scatter plot visualizes the relationship between housing growth (measured as permits per 1000 people in 2023) and population growth from 2014 to 2023 for various CBSAs. The dashed lines indicate the median housing growth and zero population growth. CBSAs in the top-right quadrant show desirable YIMBY characteristics of both population growth and above-average housing growth."
  },
  {
    "objectID": "mp02.html#policy-brief",
    "href": "mp02.html#policy-brief",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Policy Brief",
    "text": "Policy Brief\n\nIntroduction\n\nHousing affordability is a critical issue in many metropolitan areas across the United States. Rent prices are growing faster than incomes, leading to increased rent burden for many households. This is not a sustainable situation and requires immediate attention from policymakers. Despite the challenges, some metropolitan areas have successfully implemented policies that have led to decreased rent burden while still experiencing population growth. These areas can serve as models for other cities facing similar issues.\n\n\n\nProposed Sponsors\n\nWe would like to reach out to two sponsors for this policy brief: a sponsor from Columbus, GA-AL Metropolitan Area in a YIMBY zone and a co-sponsor from Fresno, CA Metropolitan Area in a NIMBY zone. Both these metropolitan areas have successfully lowered rent burdens while growing in population and opportunities when it comes to housing affordability. By collaborating with sponsors from both YIMBY and NIMBY zones, we can gain a comprehensive understanding of the issue and develop effective strategies to address it across America.\n\n\n\nBenefits\n\nTrade, Transportation, and Utilities (NAICS 42-49)\n\nBy supporting affordable housing initiatives in Columbus, GA-AL Metropolitan Area and Fresno, CA Metropolitan Area, we can ensure that workers in the Trade, Transportation, and Utilities sector have access to affordable housing options. This will not only improve their quality of life but also enhance their productivity and job satisfaction, leading to a more robust economy in these metropolitan areas. Lower housing costs and reducing rent near transit reduces commute time and stress, which improves overall well-being and reliability. Recruitment agencies will save money on recruitment and retention costs as employees are more likely to stay in jobs where they can afford to live comfortably.\n\n\n\nHealthcare and Social Assistance (NAICS 62)\n\nAffordable housing is crucial for healthcare and social assistance workers, who often face high rent burdens. By investing in affordable housing initiatives in Columbus, GA-AL Metropolitan Area and Fresno, CA Metropolitan Area, we can ensure that these essential workers have access to safe and affordable housing options. This will not only improve their quality of life but also enhance their ability to provide quality care to the community. Affordable housing near healthcare facilities can lead to better health outcomes for both workers and patients, as it reduces stress and improves access to care. This can result in lower healthcare costs and improved public health in these metropolitan areas.\n\n\n\n\nMetrics\n\nRent Burden Index (RBI): is a 0-100 index scale using min-max normalization to standardize rent burden across CBSAs using 2014 as a baseline. A score of 0 indicates the lowest rent burden observed, which is a good thing, while a score of 100 represents the highest rent burden meaning too much income is being spent on rent. This metric allows for easy comparison of rent burdens across different metropolitan areas.\nInstantaneous Housing Growth Score: This score measures the rate of housing growth in a metropolitan area compared to the average housing growth in 2014. A score above 100 indicates that the area is building housing at a faster rate than the 2014 average, while a score below 100 indicates slower growth. This metric helps identify areas that are actively addressing housing shortages.\n\n\n\nConclusion\n\nBy focusing on metropolitan areas like Columbus, GA-AL and Fresno, CA, which have demonstrated success in reducing rent burdens while experiencing population growth, we can develop effective strategies to address housing affordability across the United States. Investing in affordable housing initiatives will not only improve the quality of life for residents but also contribute to the overall economic health of these metropolitan areas. We urge policymakers to consider these metrics and examples when formulating housing policies to ensure that all residents have access to safe and affordable housing options."
  },
  {
    "objectID": "mp02.html#initial-visualizations",
    "href": "mp02.html#initial-visualizations",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Initial Visualizations",
    "text": "Initial Visualizations\nThe relationship between monthly rent and average household income per CBSA in 2009.\n\n\nCode\np1 &lt;- ggplot(combined_data |&gt; filter(year == 2009), \n             aes(x = household_income, y = monthly_rent)) +\n    geom_point(alpha = 0.6, color=\"blue\") +\n    geom_smooth(method = \"lm\", color = \"#F7B800\") +\n    labs(title = \"Relationship between Monthly Rent and Average Household Income (2009)\",\n         x = \"Average Household Income\",\n         y = \"Monthly Rent\",\n        caption = \"Source: US Census Bureau ACS\") +\n    theme_grey( base_size = 18) + \n    scale_x_continuous(labels = scales::dollar_format()) +\n    scale_y_continuous(labels = scales::dollar_format()) +\n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\nprint(p1)\n\n\n\n\n\n\n\n\n\n\nThe scatter plot shows a strong positive relationship between household income and monthly rent in 2009. Metro areas tend to have higher average household incomes which results in higher monthly rents. The linear regression line indicates that as household income increases, monthly rent also tends to increase.\n\nThe relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs. Design your visualization so that it is possible to see the evolution of this relationship over time.\n\n\nCode\n# get the total employment per year per cbsa\nemp_total &lt;- WAGES |&gt; \n    group_by(YEAR) |&gt; \n  summarise(total_employment = sum(EMPLOYMENT)) |&gt; \n    ungroup()\n\n# get total employment of health\nhealth_total &lt;- WAGES |&gt; \n    filter(str_starts(as.character(INDUSTRY), \"62\")) |&gt; \n    group_by(YEAR) |&gt; \n    summarise(health_employment = sum(EMPLOYMENT)) |&gt;\n    ungroup()\n# join both and calculate fraction\ngrouped_emp &lt;- inner_join(emp_total, health_total, by = \"YEAR\") |&gt; \n    mutate(fraction_health_employment = health_employment / total_employment)\n\np2 &lt;- ggplot(grouped_emp, aes(x= YEAR, y =fraction_health_employment)) +\n    geom_line(size = 0.8, color = \"#F7B800\") +\n    geom_point(color = \"blue\") +\n    labs(title = \"Total and Health Employment Percantage over time\",\n         x = \"Year\",\n         y = \"Percentage of Health Employment to Total Employment\",\n         caption = \"Source: BLS QCEW\") +\n    theme_grey(base_size = 18) + \n    scale_y_continuous(labels = scales::percent_format(accuarcy = 1)) +\n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\nprint(p2)\n\n\n\n\n\n\n\n\n\n\nThe plot shows an increase of percentage of Health Employees to Total Employees over time. This indicates that the health care and social services sector has been growing in the economy. There was noticeable growth from 2019 to 2021 due to the pandemic.\n\nThe evolution of average household size over time. Use different lines to represent different CBSAs.\n\n\nCode\np3 &lt;- ggplot(combined_data, aes(x= year, y = population / households, \n                                group = GEOID, \n                                color =case_when(GEOID == 35620 ~ \"New York\",\n                                                 GEOID == 31080 ~ \"Los Angeles\"))) +\n    geom_line(linewidth = 0.8) +\n   scale_color_manual(values = c(\"New York\" = \"#F7B800\", \"Los Angeles\" = \"green\"),\n                     name = \"City\") +\n  gghighlight(GEOID %in% c(35620, 31080), \n              use_direct_label = FALSE, \n              unhighlighted_params = list(alpha=0.5)) +\n    labs(title = \"Average Household Size over Time by CBSA\",\n         x = \"Year\",\n         y = \"Average Household Size\",\n         caption = \"Source: US Census Bureau ACS\") +\n    theme_grey(base_size = 18) + \n    theme(plot.title = element_text(size = 12, face = \"bold\"),\n          plot.caption = element_text(size = 5),\n          axis.title = element_text(size = 9),\n          axis.text = element_text(size = 7))\n\nprint(p3)\n\n\n\n\n\n\n\n\n\n\nThe plot shows the average household size over time for different CBSAs, with a focus on New York and Los Angeles. Both cities show a slight decrease in average household size over the years, indicating a trend towards smaller households in these metropolitan areas."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "Urban trees play a crucial role in enhancing the quality of life in cities by providing shade, improving air quality, and supporting biodiversity. In New York City, the distribution and health of urban trees vary significantly across different council districts. This mini-project aims to analyze the distribution of trees in NYC’s council districts, identify areas with high tree mortality rates, and propose strategies for maintaining and enhancing the urban canopy."
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "Urban trees play a crucial role in enhancing the quality of life in cities by providing shade, improving air quality, and supporting biodiversity. In New York City, the distribution and health of urban trees vary significantly across different council districts. This mini-project aims to analyze the distribution of trees in NYC’s council districts, identify areas with high tree mortality rates, and propose strategies for maintaining and enhancing the urban canopy."
  },
  {
    "objectID": "mp03.html#data-acquisition-and-preparation",
    "href": "mp03.html#data-acquisition-and-preparation",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Data Acquisition and Preparation",
    "text": "Data Acquisition and Preparation\nCode related to Data Acquisition and Preparation goes here.\nRemember that code blocks look like this:\n\n\nCode\n# Your code goes in chunks like this\nlibrary(tidyverse) # You will want this line for almost all MPs\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nx &lt;- 1 + 2 + 3\n\n\nand you can then print variables from chunks:\n\n\nCode\nx\n\n\n[1] 6\n\n\nor inline, like this: \\(x\\) is 6."
  },
  {
    "objectID": "mp03.html#exploratory-analysis",
    "href": "mp03.html#exploratory-analysis",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nCode related to Exploratory Data Analysis goes here. This may include exploratory graphics, instructor-provided exploratory questions, or other similar elements."
  },
  {
    "objectID": "mp03.html#final-insights-and-deliverable",
    "href": "mp03.html#final-insights-and-deliverable",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Final Insights and Deliverable",
    "text": "Final Insights and Deliverable\nCode related to the final deliverable of the assignment goes here.\n\nThis work ©2025 by iouie was initially prepared as a Mini-Project for STA 9750 at Baruch College. More details about this course can be found at the course site and instructions for this assignment can be found at MP #03"
  },
  {
    "objectID": "mp03.html#task-1-data-acquisition-and-preparation",
    "href": "mp03.html#task-1-data-acquisition-and-preparation",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 1 Data Acquisition and Preparation",
    "text": "Task 1 Data Acquisition and Preparation\nWrite code to download data from the NYC Open Data API or other sources, and prepare it\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(sf)\nlibrary(fs)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(plotly)\nlibrary(DT)\nlibrary(htmlwidgets)\n\n\n# create directory mp3 if it doesn't exist\nif(!dir.exists(file.path(\"data\", \"mp03\"))){\n    dir.create(file.path(\"data\", \"mp03\"), showWarnings=FALSE, recursive=TRUE)\n}\n\n# download current file\nget_url &lt;- glue(\"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\")\n\ndownload.file(get_url, destfile = file.path(\"data\", \"mp03\", \"nycc_25c.zip\"))\n\n# unzip\nunzip(file.path(\"data\", \"mp03\", \"nycc_25c.zip\"), exdir = file.path(\"data\", \"mp03\"))\n\n# read in data\nread_data &lt;- st_read(file.path(\"data\", \"mp03\", \"nycc_25c\", \"nycc.shp\"))\n\n#transform file to WGS 84\ntransformed_data &lt;- st_transform(read_data, crs = \"WGS84\")\n\n\n# Grabbing Forestry Tree Points API\n# https://data.cityofnewyork.us/resource/hn5i-inap.geojson\n\n# check if 200 resp\n\n# function to get tree data\nfetch_tree_data &lt;- function(url){\n  mp03 &lt;- file.path(\"data\", \"mp03\")\n  limit &lt;- 5000\n  offset &lt;- 0\n  page &lt;- 1\n  bool &lt;- TRUE\n  \n  while (bool){\n  # name\n  name &lt;- file.path(mp03, paste0(\"tree_data\", page, \".geojson\"))\n  \n  if(!file_exists(name)){\n  # build request\n    request(url) |&gt;\n    req_url_query(`$limit` = limit,`$offset` = offset) |&gt;\n    req_perform() |&gt;\n    resp_body_raw() |&gt;\n    writeBin(name)\n  }\n       \n   n_row &lt;- if (!is.null(st_read(name, quiet = TRUE))) {\n                nrow(st_read(name, quiet = TRUE))}\n             else 0\n\n    if (n_row &lt; limit) {\n      bool &lt;- FALSE} \n    else {\n      offset &lt;- offset + limit\n      page &lt;- page + 1}\n  }\n  \n  geo_file &lt;- dir_ls(mp03, glob = \"*.geojson\")\n  sflist &lt;- lapply(as.character(geo_file), st_read, quiet = TRUE) |&gt;\n  lapply(mutate, planteddate = as.character(planteddate))\n  \n  res &lt;- bind_rows(sflist)\n  \n  return(res)\n}\n\ntree &lt;- fetch_tree_data(\"https://data.cityofnewyork.us/resource/hn5i-inap.geojson\")"
  },
  {
    "objectID": "mp03.html#task-3",
    "href": "mp03.html#task-3",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 3",
    "text": "Task 3\n\n\nCode\n# Number of trees in each district\ntrees_sf &lt;- st_transform(tree, crs = st_crs(transformed_data))\n\nhits &lt;- st_intersects(transformed_data, trees_sf)\n\ntransformed_data &lt;- transformed_data |&gt;\n  mutate(tree_count = lengths(hits))\n\ntransformed_data &lt;- transformed_data |&gt;\n  mutate(hover = paste0(\"District: \", CounDist, \"&lt;br&gt;Trees: \", tree_count))\n\ntransformed_data |&gt; group_by(CounDist) |&gt; summarize(n = n(), do_union=FALSE)\n\n\n tree_plot &lt;- ggplot(transformed_data) + \n  geom_sf(aes(fill = tree_count, text = hover), color = \"black\", size = 0.8) +\n   scale_fill_viridis_c(option = \"viridis\", direction = -1, name = \"Tree density\") +\n  xlab(\"Longitude\") + \n  ylab(\"Latitude\") + \n  labs(title = \"NYC Trees in the City Council Districts\") +\n  theme_bw()\n\np &lt;- ggplotly(tree_plot, tooltip = \"text\")\n\nsaveWidget(as_widget(p), \"docs/tree_plot.html\", selfcontained = TRUE)"
  },
  {
    "objectID": "mp03.html#task-4",
    "href": "mp03.html#task-4",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 4",
    "text": "Task 4"
  },
  {
    "objectID": "mp03.html#nyc-parks-proposal",
    "href": "mp03.html#nyc-parks-proposal",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "NYC Parks Proposal",
    "text": "NYC Parks Proposal\n\nSummary\n\nWe are proposing a tree planting initiative in council district 32, which has the highest fraction of dead trees at 14.22%. By planting new trees in this district, we can improve the overall health of the urban forest, enhance air quality, and provide shade and aesthetic value while maintaining a nature vibe to the community.\n\n\n\nScope\n\nRemove dead trees replace them with new saplings.\nFocus on planting native species that are well-suited to the local environment.\n\n\n\nComparison\n\nBar chart comparing another district with similar tree density (31)\nNeighboring district (34)\nDistrict (39)\n\n\n\nCode\ncompare_trees &lt;- counts_dead |&gt;\n  filter(CounDist %in% c(31, 32, 37, 39)) |&gt;\n  mutate(\n    district = case_when(\n      CounDist == 31 ~ \"Queens 31\",\n      CounDist == 32 ~ \"Queens 32\",\n      CounDist == 37 ~ \"Queens 37\",\n      CounDist == 39 ~ \"Brooklyn 39\",\n      TRUE ~ as.character(CounDist)\n    ),\n    Pct_Dead = as.numeric(sub(\"%\", \"\", Pct_Dead)\n  )) |&gt;\n  select(district, Pct_Dead, Num_Trees, Total_Trees)\n\nggplot(compare_trees, aes(x = district, y = Pct_Dead, fill = district)) +\n  geom_col() +\n  geom_text(aes(label = Pct_Dead), vjust = -0.5) +\n  labs(title = \"Percentage of Dead Trees in Selected Districts\",\n       x = \"District\",\n       y = \"Dead Trees(%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(compare_trees, aes(x = district, y = Total_Trees, fill = district)) +\n  geom_col() +\n  geom_text(aes(label = Total_Trees), vjust = -0.5) +\n  labs(title = \"Total Number of Trees in Selected Districts\",\n       x = \"District\",\n       y = \"Total Trees\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBoth Brooklyn District 39 and Queens District 31 have similar total # of trees as 32, but have relatively less percentage of dead trees. Queens District 37 is a neighboring district with a lower percentage of dead trees as well.\n\n\n\nCode\nchosen_districts &lt;- unique_trees |&gt;\n  mutate(CounDist = as.integer(CounDist)) |&gt;\n  filter(CounDist %in% c(31, 32, 37, 39)) |&gt;\n  group_by(CounDist, tree_type) |&gt;\n  summarise(count = sum(.data$n, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(CounDist, desc(count))\n\ntop1_per_district &lt;- chosen_districts %&gt;%\n  group_by(CounDist) %&gt;%\n  slice_max(order_by = count, n = 1, with_ties = FALSE) %&gt;%  # with_ties = TRUE would keep ties\n  ungroup() %&gt;%\n  mutate(\n    district_label = case_when(\n      CounDist == 31 ~ \"Queens 31\",\n      CounDist == 32 ~ \"Queens 32\",\n      CounDist == 37 ~ \"Queens 37\",\n      CounDist == 39 ~ \"Brooklyn 39\",\n      TRUE ~ as.character(CounDist)\n    ),\n    district_label = fct_reorder(district_label, count, .desc = TRUE)\n  )\n\nggplot(top1_per_district, aes(x = district_label, y = count, fill = district_label)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = paste0(tree_type, \" (\", count, \")\")), \n            hjust = -0.05, size = 3) +\n  coord_flip() +\n  labs(title = \"Top species by count — selected districts\",\n       x = \"District\", y = \"Highest # of Genus Species\") +\n  theme_minimal() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15)))\n\n\n\n\n\n\n\n\n\nCode\n datatable(chosen_districts,\n    class = \"table table-striped table-hover table-dark table-sm\",\n    caption = \"Top Trees in Selected district\",\n    options = list(pageLength = 10, searching = FALSE, lengthChange = FALSE))\n\n\n\n\n\n\n\nBoth Brooklyn District 39 and Queens District 32 have Platanus x acerifolia - London planetree as their most common tree species, but district 32 has higher tree death percentages. Their neighboring district, Queens District 37 and Queens District 31, has Gleditsia triacanthos var. inermis - Thornless honeylocust as the most common species. If there were more of what District 31 and District 37 had, perhaps District 32 could reduce its tree death rate."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "We are intending to look at the CES revision dating from 1979 to 2025 and see if there are any patterns in the revisions. We will also fact-check some claims made by politicians regarding the CES revisions."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "We are intending to look at the CES revision dating from 1979 to 2025 and see if there are any patterns in the revisions. We will also fact-check some claims made by politicians regarding the CES revisions."
  },
  {
    "objectID": "mp04.html#data-acquisition-and-preparation",
    "href": "mp04.html#data-acquisition-and-preparation",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Data Acquisition and Preparation",
    "text": "Data Acquisition and Preparation\nCode related to Data Acquisition and Preparation goes here.\nRemember that code blocks look like this:"
  },
  {
    "objectID": "mp04.html#exploratory-analysis",
    "href": "mp04.html#exploratory-analysis",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nCode related to Exploratory Data Analysis goes here. This may include exploratory graphics, instructor-provided exploratory questions, or other similar elements."
  },
  {
    "objectID": "mp04.html#final-insights-and-deliverable",
    "href": "mp04.html#final-insights-and-deliverable",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Final Insights and Deliverable",
    "text": "Final Insights and Deliverable\nCode related to the final deliverable of the assignment goes here.\n\nThis work ©2025 by iouie was initially prepared as a Mini-Project for STA 9750 at Baruch College. More details about this course can be found at the course site and instructions for this assignment can be found at MP #04"
  },
  {
    "objectID": "mp04.html#data-acquisition-and-preparation-web",
    "href": "mp04.html#data-acquisition-and-preparation-web",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "Data Acquisition and Preparation Web",
    "text": "Data Acquisition and Preparation Web\nWebscraping\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(rvest)\nlibrary(httr2)\n\n\nWarning: package 'httr2' was built under R version 4.5.2\n\n\nCode\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\nlibrary(stringr)\nlibrary(readr)\n\n\n\nAttaching package: 'readr'\n\n\nThe following object is masked from 'package:rvest':\n\n    guess_encoding\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp04\"))) {\n  dir.create(file.path(\"data\", \"mp04\"), showWarnings = FALSE, recursive = TRUE)\n}\n\n\n#1- Get the URL\nbase_url &lt;- \"https://data.bls.gov/pdq/SurveyOutputServlet\"\n\n#2- Grab Payload\nform_list &lt;- list(\n    request_action = \"get_data\",\n    reformat = \"true\",\n    from_results_page = \"true\",\n    from_year = \"1979\",\n    to_year = \"2025\",\n    Go.x = \"11\",\n    Go.y = \"10\",\n    initial_request = \"false\",\n    data_tool = \"surveymost\",\n    series_id = \"CES0000000001\",\n    year_options = \"specific_years\"\n  ) \n#3 - Get table\nces_req &lt;- request(base_url) |&gt;\n  req_method(\"POST\") |&gt;\n  req_body_form(!!!form_list) |&gt;\n  req_perform() |&gt;\n  resp_body_html() |&gt;\n  html_elements(\"#table0\") |&gt;\n  html_table(fill = TRUE)\n\nhead(ces_req)\n\n\n[[1]]\n# A tibble: 48 × 13\n   Year  Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec  \n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1979  88808 89055 89479 89417 89789 90108 90217 90300 90327 90481 90573 90672\n 2 1980  90800 90883 90994 90849 90420 90101 89840 90099 90213 90490 90747 90943\n 3 1981  91033 91105 91210 91283 91296 91490 91601 91565 91477 91380 91171 90895\n 4 1982  90565 90563 90434 90150 90107 89865 89521 89363 89183 88907 88786 88771\n 5 1983  88990 88917 89090 89364 89644 90021 90437 90129 91247 91520 91875 92230\n 6 1984  92673 93157 93429 93792 94098 94479 94789 95032 95344 95629 95982 96107\n 7 1985  96372 96503 96842 97038 97312 97459 97648 97840 98045 98233 98443 98609\n 8 1986  98732 98847 98934 99121 99248 99155 99473 99588 99934 1001… 1003… 1005…\n 9 1987  1006… 1009… 1011… 1014… 1017… 1019… 1022… 1024… 1026… 1031… 1033… 1036…\n10 1988  1037… 1042… 1044… 1047… 1049… 1053… 1055… 1056… 1060… 1062… 1066… 1068…\n# ℹ 38 more rows\n\n\nCode\n#4 -  Clean table and convert\nces_clean &lt;- ces_req[[1]] |&gt;\n  pivot_longer(cols = -Year, names_to = \"month\", values_to = \"level\") |&gt;\n  mutate(\n    month = str_squish(str_remove_all(month, \"\\\\*\")),\n    \n    date = ym(paste(Year, month)),\n    \n    level = parse_number(level)\n  ) |&gt;\n  drop_na(level, date) |&gt;\n  arrange(date) |&gt;\n  select(date, month, level)\n\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `date = ym(paste(Year, month))`.\nCaused by warning:\n!  12 failed to parse.\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\nCode\nhead(ces_clean)\n\n\n# A tibble: 6 × 3\n  date       month level\n  &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt;\n1 1979-01-01 Jan   88808\n2 1979-02-01 Feb   89055\n3 1979-03-01 Mar   89479\n4 1979-04-01 Apr   89417\n5 1979-05-01 May   89789\n6 1979-06-01 Jun   90108"
  },
  {
    "objectID": "mp04.html#data-acquisition",
    "href": "mp04.html#data-acquisition",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "2) Data Acquisition",
    "text": "2) Data Acquisition\nWe are acquiring data from the Bureau of Labor Statistics (BLS) website. The data consists of two parts: the total employment data and the revisions data.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rvest)\nlibrary(httr2)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(infer)\nlibrary(scales)\n\nif(!dir.exists(file.path(\"data\", \"mp04\"))) {\n  dir.create(file.path(\"data\", \"mp04\"), showWarnings = FALSE, recursive = TRUE)\n}\n\n\n#1 - Get the URL\nbase_url &lt;- \"https://data.bls.gov/pdq/SurveyOutputServlet\"\n\n#2 - Grab Payload\nform_list &lt;- list(\n    request_action = \"get_data\",\n    reformat = \"true\",\n    from_results_page = \"true\",\n    from_year = \"1979\",\n    to_year = \"2025\",\n    Go.x = \"11\",\n    Go.y = \"10\",\n    initial_request = \"false\",\n    data_tool = \"surveymost\",\n    series_id = \"CES0000000001\",\n    year_options = \"specific_years\"\n  ) \n#3 - Get table\nces_req &lt;- request(base_url) |&gt;\n  req_method(\"POST\") |&gt;\n  req_body_form(!!!form_list) |&gt;\n  req_perform() |&gt;\n  resp_body_html() |&gt;\n  html_elements(\"#table0\") |&gt;\n  html_table(fill = TRUE)\n\nhead(ces_req)\n\n#4 -  Clean table and convert\nces_clean &lt;- ces_req[[1]] |&gt;\n  pivot_longer(cols = -Year, names_to = \"month\", values_to = \"level\") |&gt;\n  mutate(\n    month = str_squish(str_remove_all(month, \"\\\\*\")),\n    \n    date = ym(paste(Year, month)),\n    \n    level = parse_number(level)\n  ) |&gt;\n  drop_na(level, date) |&gt;\n  arrange(date) |&gt;\n  select(date, month, level)\n\nhead(ces_clean)"
  },
  {
    "objectID": "mp04.html#ces-revisions",
    "href": "mp04.html#ces-revisions",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "3) CES Revisions",
    "text": "3) CES Revisions\nWe are downloading the CES Non-Farm Payroll Employment numbers.\n\n\nCode\n#1 - Request headers\nresp &lt;- request(\"https://www.bls.gov/web/empsit/cesnaicsrev.htm\") |&gt;\n  req_user_agent(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36\"\n  ) |&gt;\nreq_headers(\n  \"Accept\" = \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n  \"Accept-Language\" = \"en-US,en;q=0.9\",\n  \"Cache-Control\" = \"max-age=0\",\n  \"Referer\" = \"https://michael-weylandt.com/\",\n  \"Cookie\" = \"_ga=GA1.1.2096679178.1761100449; nmstat=46b4296a-7687-2f1d-769d-eb9152d5657c\"\n) |&gt;\n  req_perform() |&gt;\n  resp_body_html()\n\n#2 - Get Year Function\nget_year &lt;- function(year, html) {\n  \n  tbl_id &lt;- paste0(\"#\", year)\n  \n  tbl &lt;- html |&gt; \n    html_element(tbl_id)\n  \n  if(is.na(tbl_id)) return(NULL)\n  \n  tbl &lt;- tbl |&gt;\n    html_element(\"tbody\") |&gt;\n    html_table(fill = TRUE, header = FALSE)\n  \n  tbl |&gt; \n    slice(1:12) |&gt;\n    select(\n      month = 1,\n      original = 3,\n      final = 5\n    ) |&gt;\n    mutate(\n      month = str_trim(month),\n      date = ym(paste(year, month)),\n      original = as.numeric(gsub(\"[^0-9-]\", \"\", original)),\n      final = as.numeric(gsub(\"[^0-9-]\", \"\", final)),\n      revision = final - original\n    ) |&gt;\n    drop_na()\n}\n\n#3 - Apply Revision\nces &lt;- map(1979:2025, ~get_year(.x, resp)) |&gt;\n  list_rbind() |&gt;\n  select(date, original, final, revision)\n\nhead(ces)"
  },
  {
    "objectID": "mp04.html#data-integration-and-exploration",
    "href": "mp04.html#data-integration-and-exploration",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "4) Data Integration and Exploration",
    "text": "4) Data Integration and Exploration\nNow we join these 2 tables together and find some interesting insights. Let’s look at the unemployment levels from 1979 until 2025.\n\n\nCode\nces_full &lt;- full_join(ces_clean, ces, by = \"date\") |&gt;\n  arrange(date) |&gt;\n    select(-month)\n\n# grab year and unemployment levels\nces_clean &lt;- ces_clean |&gt;\n  mutate(date = as.Date(date, format = \"%Y-%m-%d\"),\n         year = year(date)) |&gt;\n  group_by(year) |&gt;\n  summarize(\n    employment_level = sum(level, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nggplot(ces_clean, aes(x = year, y = employment_level)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"steelblue\") +\n  labs(\n    title = \"Total Nonfarm Employment Over Time\",\n    x = \"Year\",\n    y = \"Total Nonfarm Employment Level(Millions)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ndatatable(ces_clean,\n           class = \"table table-striped table-hover table-dark table-sm\",\n          caption=\"Total Nonfarm Employment overtime\",\n          options = list(pageLength = 10, searching = FALSE, lengthChange = FALSE)\n          )\n\n\n\n4.1) What and when were the largest revisions (positive and negative in CES history)?\n\n\nCode\np1 &lt;- ggplot(ces_full, aes(x = date, y = revision)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"CES Revisions Over Time\",\n    x = \"Date\",\n    y = \"Revision (Final - Original)\"\n  ) +\n  theme_minimal()\n\nces_tbl1 &lt;- ces_full |&gt;\n  select(date, original, final, revision) |&gt;\n  arrange(desc(abs(revision))) |&gt;\n  drop_na()\n\nprint(p1)\n\n\n\n\n\n\n\n\n\nCode\ndatatable(ces_tbl1,\n           class = \"table table-striped table-hover table-dark table-sm\",\n          caption=\"Table of revisions over time\",\n          options = list(pageLength = 10, searching = FALSE, lengthChange = FALSE)\n          )\n\n\n\nThe largest negative revision occurred in March 2020, with a revision of -672,000 jobs which is likely due to the initial impact of the COVID-19 pandemic on employment figures. The largest positive revision occurred in November 2020, with a revision of 437,000 jobs, reflecting a significant upward adjustment as the economy began to recover from the initial shock of the pandemic.\n\n\n\n4.2) What fraction of CES revisions are positive in each decade?\n\n\nCode\nces_tbl2 &lt;- ces_full |&gt;\n  mutate(\n    decade = case_when(\n      year(date) &lt; 1990 ~ \"1980s\",\n      year(date) &lt; 2000 ~ \"1990s\",\n      year(date) &lt; 2010 ~ \"2000s\",\n      year(date) &lt; 2020 ~ \"2010s\",\n      TRUE ~ \"2020s\"\n    )\n  ) |&gt;\n  group_by(decade) |&gt;\n  summarise(\n    num_revisions = n(),\n    num_revisions_positive = sum(revision &gt; 0, na.rm = TRUE),\n    pct_pos = round(num_revisions_positive / num_revisions * 100, 2)\n  ) |&gt;\n  arrange(decade)\n\ndatatable(ces_tbl2,\n           class = \"table table-striped table-hover table-dark table-sm\",\n          caption=\"Positive Revisions in each decade\",\n          options = list(pageLength = 5, searching = FALSE, lengthChange = FALSE)\n          )\n\n\n\nThe 1990’s has the highest fraction of positive revisions at 69.17%, while the 2020’s has the lowest at 44.93%. This is a worrying trend as it indicates that more recent CES data revisions tend to be negative, suggesting an overall economic downturn or possibly a recession.\n\n\n\n4.3) How has the relative CES revision magnitude (absolute value of revision amount over final estimate) changed over time?\n\n\nCode\nces_tbl3 &lt;- ces_full |&gt;\n  mutate(\n    rel_revision = abs(revision / final),\n    rel_revision = if_else(rel_revision == Inf, NA_real_, rel_revision)\n  ) \n  \n\np2 &lt;- ggplot(ces_tbl3, aes(x = date, y = rel_revision)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"CES Revisions Magnitude Over Time\",\n    x = \"Date\",\n    y = \"Revision Magnitude abs(revision / final)\"\n  ) +\n  theme_minimal()\n\nprint(p2)\n\n\n\n\n\n\n\n\n\nCode\ndatatable(ces_tbl3,\n           class = \"table table-striped table-hover table-dark table-sm\",\n          caption=\"Revisions Magnitude over time\",\n          options = list(pageLength = 5, searching = FALSE, lengthChange = FALSE)\n          )\n\n\n\nThe relative revision magnitude appears to have increased during periods of economic uncertainty, such as the early 2000s recession and the COVID-19 pandemic in 2020. This suggests that during times of economic stress, the initial CES estimates are more likely to be significantly revised as more accurate data becomes available. In the 2020’s, it looks to be rising due to economic uncertainty.\n\n\n\n4.4) How has the absolute CES revision as a percentage of overall employment level changed over time?\n\n\nCode\nces_tbl4 &lt;- ces_full |&gt;\n  mutate(\n    pct_revision = abs(round(revision / final * 100,2)),\n    pct_revision = if_else(pct_revision == Inf, NA_real_, pct_revision)\n  )\n\np3 &lt;- ggplot(ces_tbl4, aes(x = date, y = pct_revision)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"CES Revision Percent over Time\",\n    x = \"Date\",\n    y = \"Revision Percentage over Time\"\n  ) +\n  theme_minimal()\n\nprint(p3)\n\n\n\n\n\n\n\n\n\nCode\ndatatable(ces_tbl4,\n           class = \"table table-striped table-hover table-dark table-sm\",\n          caption=\"Revisions Percentage over Time\",\n          options = list(pageLength = 10, searching = FALSE, lengthChange = FALSE)\n          )\n\n\n\nOther than the outlier from 1990’s and the 2020’s,the absolute CES revision as a percentage has been relatively stable until recent years where it appears have spiked. This is also around the same time when Trump fired the Labor statistics chief, Erika McEntarfer.\n\n\n\n4.5) Are there any months that systematically have larger or smaller CES revisions?\n\n\nCode\nces_tbl5 &lt;- ces_full |&gt;\n    mutate(\n    date = as.Date(date),\n    year = year(date),\n    month = month(date),                        \n    month_name = factor(month.abb[month],       \n                        levels = month.abb),\n    abs_rev = abs(revision),\n    final_abs = abs(final),\n    final_safe = if_else(final_abs &lt; 1e-8, NA_real_, final_abs),\n    abs_rev_pct = abs_rev / final_safe * 100,\n    abs_rev_pct_capped = pmin(abs_rev_pct, 1000)\n  ) \n\np4 &lt;- ggplot(ces_tbl5, aes(x = month_name, y = abs_rev_pct_capped)) +\n  geom_boxplot(outlier.colour = \"grey50\", na.rm = TRUE) +\n  geom_jitter(width = 0.15, alpha = 0.3, size = 0.8, na.rm = TRUE) +\n  labs(x = \"Month\", y = \"Absolute revision (% of final)\", title = \"CES absolute revision by month\") +\n  theme_minimal()\n\nprint(p4)\n\n\n\n\n\n\n\n\n\n\nLooking at the box plot, it appears that September have a larger CES revision. That’s relatively close to election month, perhaps the numbers are spiked up to make a re-election more favorable. December seems to have a smaller revision within the boxplot.\n\n\n\n4.6) How large is the average CES revision in absolute terms? In terms of percent of that month’s CES level?\n\n\nCode\nces_tbl6 &lt;- ces_tbl5 |&gt;\n   filter(is.finite(abs_rev_pct)) %&gt;% \n  group_by(month, month_name) %&gt;%\n  summarise(\n    n = n(),\n    n_years = n_distinct(year(date)),\n    mean_pct = mean(abs_rev_pct, na.rm = TRUE),\n    median_pct = median(abs_rev_pct, na.rm = TRUE),\n    sd_pct = sd(abs_rev_pct, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(month) %&gt;%\n  mutate(\n    mean_pct_round = round(mean_pct, 2),\n    median_pct_round = round(median_pct, 2),\n    mean_label = paste0(formatC(mean_pct_round, format = \"f\", digits = 2), \"%\")\n  )\n\np5 &lt;- ggplot(ces_tbl6, aes(x = month_name, y = mean_pct)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = mean_label), vjust = -0.5, size = 3) +\n  labs(x = \"Month\", y = \"Average absolute revision (%)\",\n       title = \"Average absolute CES revision by calendar month\") +\n  theme_minimal()\n\nprint(p5)\n\n\n\n\n\n\n\n\n\n\nSeptember has the highest and November has the lowest."
  },
  {
    "objectID": "mp04.html#statistical-inference",
    "href": "mp04.html#statistical-inference",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "5) Statistical Inference",
    "text": "5) Statistical Inference\n\n5.1) Is the average revision significantly different from zero?\n\n\nCode\n# Prep data\n\nces_prep &lt;- ces_full |&gt;\n  mutate(\n  date = as.Date(date),\n  year = year(date),\n  is_negative = if_else(!is.na(revision), revision &lt; 0, NA),\n  pre2000 = if_else(year &lt; 2000, \"pre\", \"post\"),\n  )\n\n#t=test\nt.test(ces_prep$revision, mu = 0, alternative = \"two.sided\")\n\n\n\n    One Sample t-test\n\ndata:  ces_prep$revision\nt = 3.2588, df = 558, p-value = 0.001187\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  4.558826 18.392874\nsample estimates:\nmean of x \n 11.47585 \n\n\n\nThe one-sample t-test shows increase in initial employment by 11,475 jobs per month. Since the p-value is 0.00132, which is less than the significance level of 0.05, we reject the null hypothesis and conclude that there is a statistically significant difference from zero in the average CES revision.\n\n\n\n5.2) Has the fraction of negative revisions increased post-2000?\n\n\nCode\nx_pre  &lt;- sum(ces_prep$is_negative == 1 & ces_prep$pre2000 == \"pre\",  na.rm = TRUE)\nn_pre  &lt;- sum(!is.na(ces_prep$is_negative) & ces_prep$pre2000 == \"pre\")\nx_post &lt;- sum(ces_prep$is_negative == 1 & ces_prep$pre2000 == \"post\", na.rm = TRUE)\nn_post &lt;- sum(!is.na(ces_prep$is_negative) & ces_prep$pre2000 == \"post\")\n\nprop.test(x = c(x_pre, x_post), n = c(n_pre, n_post), alternative = \"less\", correct = FALSE)\n\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(x_pre, x_post) out of c(n_pre, n_post)\nX-squared = 0.82754, df = 1, p-value = 0.1815\nalternative hypothesis: less\n95 percent confidence interval:\n -1.00000000  0.03076708\nsample estimates:\n   prop 1    prop 2 \n0.4047619 0.4429967 \n\n\n\nThis work ©2025 by iouie was initially prepared as a Mini-Project for STA 9750 at Baruch College. More details about this course can be found at the course site and instructions for this assignment can be found at MP #04"
  },
  {
    "objectID": "mp04.html#statistical-inferences",
    "href": "mp04.html#statistical-inferences",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "5) Statistical Inferences",
    "text": "5) Statistical Inferences\n\n5.1) Is the average revision significantly different from zero?\n\n\nCode\n# Prep data\n\nces_prep &lt;- ces_full |&gt;\n  mutate(\n  date = as.Date(date),\n  year = year(date),\n  is_negative = if_else(!is.na(revision), revision &lt; 0, NA),\n  pre2000 = if_else(year &lt; 2000, \"pre\", \"post\"),\n  )\n\n#t=test\nt.test(ces_prep$revision, mu = 0, alternative = \"two.sided\")\n\n\n\nThe one-sample t-test shows increase in initial employment by 11,475 jobs per month. Since the p-value is 0.00132, which is less than the significance level of 0.05, we reject the null hypothesis and conclude that there is a statistically significant difference from zero in the average CES revision.\n\n\n\n5.2) Has the fraction of negative revisions increased post-2000?\n\n\nCode\nx_pre  &lt;- sum(ces_prep$is_negative == 1 & ces_prep$pre2000 == \"pre\",  na.rm = TRUE)\nn_pre  &lt;- sum(!is.na(ces_prep$is_negative) & ces_prep$pre2000 == \"pre\")\nx_post &lt;- sum(ces_prep$is_negative == 1 & ces_prep$pre2000 == \"post\", na.rm = TRUE)\nn_post &lt;- sum(!is.na(ces_prep$is_negative) & ces_prep$pre2000 == \"post\")\n\nprop.test(x = c(x_pre, x_post), n = c(n_pre, n_post), alternative = \"less\", correct = FALSE)\n\n\n\nThe proportion of negative revisions rose from 40.48% before 2000 to 44.30% after 2000 (3.82% increase), which isn’t a significant difference. Given that X-squared is .8275, we find that the z- value is .9097 (sqrt(x-squared)) which is far from the z-value=1.96 of a two-sided test. The p-value is also greater than 0.5 so there is insufficient evidence to conclude that the fraction of negative revisions has increased post-2000."
  },
  {
    "objectID": "mp04.html#fact-check-bls-revisions",
    "href": "mp04.html#fact-check-bls-revisions",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma’am!",
    "section": "6) Fact Check BLS Revisions",
    "text": "6) Fact Check BLS Revisions\nWe are now going to look at claims politicians have made about CES revisions after the recent firing of Dr. McEntarfer using Politifacts and their scale.\nTRUE - The statement is accurate and there is nothing significant missing.\nMOSTLY TRUE - The statement is accurate but needs clarification or additional information.\nHALF TRUE - The statement is partially accurate but leaves out important details or takes things out of context.\nMOSTLY FALSE - The statement contains some element of truth but ignores critical facts that would give a different impression.\nFALSE - The statement is not accurate.\nPANTS ON FIRE - The statement is not accurate and makes a ridiculous claim.\n\n6.1) Claim 1: “Donald Trump claims the Biden Administration padded the March 2024 jobs numbers by 818,000 jobs.”\n\n“MASSIVE SCANDAL! The Harris-Biden Administration has been caught fraudulently manipulating Job Statistics to hide the true extent of the Economic Ruin they have inflicted upon America. New Data from the Bureau of Labor Statistics shows that the Administration PADDED THE NUMBERS with an extra 818,000 Jobs that DO NOT EXIST, AND NEVER DID.” - Trump Truth Social\n\n\n\nCode\n# Calculate total revision of March 2024\nces_march_2024 &lt;- ces_full |&gt;\n  filter(date == as.Date(\"2024-03-01\"))\n\nces_2024 &lt;- ces_full |&gt;\n  filter(year(date) == 2024) |&gt;\n  mutate(\n    highlight = if_else(year(date) == 2024 & month(date) == 3, \"March 2024\", \"Other Months\")\n  )\n\nggplot(ces_2024, aes(x = date, y = final, fill = highlight)) +\n  geom_col() +\n  scale_fill_manual(values= c(\"Other\" = \"black\", \"March 2024\" = \"green\"))\n\n\n\n\n\n\n\n\n\nCode\n  labs(\n    title = \"CES Revisions in 2024\",\n    x = \"Date\",\n    y = \"Final # Employment(thousands)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = comma)\n\ndatatable(ces_march_2024,\n           class = \"table table-striped table-hover table-dark table-sm\",\n          caption=\"March 2024 data\",\n          options = list(pageLength = 1, searching = FALSE, lengthChange = FALSE)\n          )\n\n\n\nAs we can see from the data, the initial employment number for March 2024 was 303,000, which was later revised to 310,000, far from the claim of padding the numbers with an extra 818,000 jobs. Therefore, this claim is rated as PANTS ON FIRE.\n\n\n\n6.2) Claim 2: There has been more negative revisions post 2000’s era than pre 2000’s era.\n\n“Since the 2000s, we’ve seen more negative BLS revisions than before — the post‑2000 era shows a clear downward tilt in the revisions” - Fake Politician 1\n\n\nNull Hypothesis H0: p_post &lt;= p_pre\nAlternative Hypothesis H1: : p_post &gt; p_pre\nFrom the t-test we conducted earlier, the negative revision rose from 40.48% pre 2000s to 44.30% after 2000s. But there’s not sufficient evidence to conclude that the fraction of negative revisions has increased post-2000 since there’s still a lot of time left. Therefore, this claim is rated as HALF TRUE."
  },
  {
    "objectID": "fp.html",
    "href": "fp.html",
    "title": "Individual Report: Streaming Service Music Trends",
    "section": "",
    "text": "Music has been an integral part of human culture for centuries, evolving through various genres and styles and one of the few mediums that connect people across different backgrounds. In the past years, the rise of digital streaming services has revolutionized the way we consume music, leading to significant changes in listening habits and industry dynamics. As the consumption of music becomes increasingly digital and data-driven, it raises an intriguing question: what factors contribute to the popularity of songs of streaming platforms?"
  },
  {
    "objectID": "fp.html#introduction",
    "href": "fp.html#introduction",
    "title": "Individual Report: Streaming Service Music Trends",
    "section": "",
    "text": "Music has been an integral part of human culture for centuries, evolving through various genres and styles and one of the few mediums that connect people across different backgrounds. In the past years, the rise of digital streaming services has revolutionized the way we consume music, leading to significant changes in listening habits and industry dynamics. As the consumption of music becomes increasingly digital and data-driven, it raises an intriguing question: what factors contribute to the popularity of songs of streaming platforms?"
  }
]